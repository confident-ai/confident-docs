---
slug: integrations/third-party/langchain
subtitle: Use Confident AI for LLM observability and evals for LangChain
---

## Overview

[LangChain](https://www.langchain.com/) is a framework for building LLM applications. Confident AI provides a `CallbackHandler` to trace and evaluate LangChain applications.

## Tracing Quickstart

<Steps>

<Step title="Install Dependencies">

Run the following command to install the required packages:

<CodeBlocks>
```python title="Python"
pip install -U deepeval langchain langchain-openai
```

```typescript title="TypeScript"
npm install langchain @langchain/core
```

</CodeBlocks>

</Step>

<Step title="Setup Confident AI Key">

Login to Confident AI using your Confident API key.

<CodeBlocks>

<CodeBlock title="Env" language="bash">

```bash
export CONFIDENT_API_KEY="<your-confident-api-key>"
```

</CodeBlock>

<CodeBlock language="bash" title="Bash">

```bash
deepeval login
```

</CodeBlock>

<CodeBlock language="python">

```python
import deepeval

deepeval.login("<your-confident-api-key>")
```

</CodeBlock>

</CodeBlocks>

</Step>

<Step title="Configure LangChain">

Provide DeepEval's `CallbackHandler` to your LangChain application's invoke method.

<CodeBlocks >

<CodeBlock language="python">

```python Python {7,29} maxLines=32
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

from deepeval.integrations.langchain import CallbackHandler

@tool
def multiply(a: int, b: int) -> int:
    """Returns the product of two numbers"""
    return a * b

llm = ChatOpenAI(model="gpt-4o-mini")

agent_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant that can perform mathematical operations."),
        ("human", "{input}"),
        MessagesPlaceholder("agent_scratchpad"),
    ]
)

agent = create_tool_calling_agent(llm, [multiply], agent_prompt)
agent_executor = AgentExecutor(agent=agent, tools=[multiply], verbose=True)

result = agent_executor.invoke(
    {"input": "What is 8 multiplied by 6?"},
    config={"callbacks": [CallbackHandler()]},
)
```

</CodeBlock>

<CodeBlock language="typescript">

```typescript TypeScript {9,11} maxLines=32
import { ChatOpenAI } from "@langchain/openai";
import {
  ChatPromptTemplate,
  MessagesPlaceholder,
} from "@langchain/core/prompts";
import { DynamicStructuredTool } from "@langchain/core/tools";
import { createToolCallingAgent, AgentExecutor } from "langchain/agents";

import { DeepEvalCallbackHandler } from "deepeval-ts/integrations";

const handler = new DeepEvalCallbackHandler({});

const multiplyTool = new DynamicStructuredTool({
  name: "multiply",
  description: "Returns the product of two numbers",
  schema: {
    a: { type: "number", description: "The first number" },
    b: { type: "number", description: "The second number" },
  },
  func: async ({ a, b }: { a: number; b: number }) => a * b,
});

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0,
});

const agentPrompt = ChatPromptTemplate.fromMessages([
  [
    "system",
    "You are a helpful assistant that can perform mathematical operations.",
  ],
  ["human", "{input}"],
  new MessagesPlaceholder("agent_scratchpad"),
]);

const agent = await createToolCallingAgent({
  llm,
  tools: [multiplyTool],
  prompt: agentPrompt,
});

const agentExecutor = new AgentExecutor({
  agent,
  tools: [multiplyTool],
  verbose: true,
});

const main = async () => {
  const result = await agentExecutor.invoke(
    { input: "What is 8 multiplied by 6?" },
    { callbacks: [handler] }
  );
};
```

</CodeBlock>

</CodeBlocks>

<Note>
  DeepEval's `CallbackHandler` extends LangChain's
  [`BaseCallbackHandler`](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html)
  or LangChain.js'
  [`BaseCallbackHandler`](https://v03.api.js.langchain.com/classes/_langchain_core.callbacks_base.BaseCallbackHandler.html).
</Note>

</Step>

<Step title="Run LangChain">

Invoke your application by executing the script:

<CodeBlocks >

<CodeBlock language="python">

```python
python main.py
```

</CodeBlock>

<CodeBlock language="typescript">

```typescript
npx ts-node
```

</CodeBlock>

</CodeBlocks>

You can directly view the traces on [Confident AI](https://app.confident.ai) by clicking on the link in the output printed in the console.

</Step>

</Steps>
## Advanced Features

### Set trace attributes

Confident AI's LLM tracing advanced features provide teams with the ability to set certain attributes for each trace when invoking your LangChain application.

For example, `thread_id` and `user_id` are used to group related traces together, and are useful for chat apps, agents, or any multi-turn interactions. You can learn more about [threads](/docs/llm-tracing/advanced-features/threads) here.

You can set these attributes in the `CallbackHandler` when invoking your LangChain application.

<CodeBlocks >

<CodeBlock language="python">

```python Python {4}
result = agent_executor.invoke(
    {"input": "What is 8 multiplied by 6?"},
    config={
        "callbacks": [CallbackHandler(thread_id="123")]
    },
)
```

</CodeBlock>

<CodeBlock language="typescript">

```typescript TypeScript {2}
const handler = new DeepEvalCallbackHandler({
  threadId: "123",
});
```

</CodeBlock>

</CodeBlocks>

<Accordion title='View Trace Attributes'>
<ParamField path="name" type="str" required={false}>
  The name of the trace. [Learn more](/docs/llm-tracing/advanced-features/name).
</ParamField>

<ParamField path="tags" type="List[str]" required={false}>
  Tags are string labels that help you group related traces. [Learn
  more](/docs/llm-tracing/advanced-features/tags).
</ParamField>

<ParamField path="metadata" type="Dict" required={false}>
  Attach any metadata to the trace. [Learn
  more](/docs/llm-tracing/advanced-features/metadata).
</ParamField>

<ParamField path="thread_id" type="str" required={false}>
  Supply the thread or conversation ID to view and evaluate conversations.
  [Learn more](/docs/llm-tracing/advanced-features/threads).
</ParamField>

<ParamField path="user_id" type="str" required={false}>
  Supply the user ID to enable user analytics. [Learn
  more](/docs/llm-tracing/advanced-features/users).
</ParamField>

<Info>
  Each attribute is **optional**, and works the same way as the [native tracing
  features](/docs/llm-tracing/introduction) on Confident AI.
</Info>
</Accordion>

### Logging prompts

If you are [managing prompts](/docs/llm-evaluation/prompt-management/version-prompts) on Confident AI and wish to log them, pass your `Prompt` object to the language model instance's `metadata` parameter.

<CodeBlocks >

<CodeBlock language="python">

```python Python
from langchain_openai import ChatOpenAI
from deepeval.prompt import Prompt

prompt = Prompt(alias="<prompt-alias>")
prompt.pull(version="00.00.01")

llm = ChatOpenAI(
    model="gpt-4o-mini",
    metadata={"prompt": prompt}
)
```

</CodeBlock>

<CodeBlock language="typescript">

```typescript TypeScript
import { ChatOpenAI } from "@langchain/openai";
import { Prompt } from "deepeval-ts";

const prompt = new Prompt({ alias: "<prompt-alias>" });
prompt.pull({ version: "00.00.01" });

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
  metadata: {
    prompt: prompt,
  },
});
```

</CodeBlock>

</CodeBlocks>

<Note>
  Logging prompts lets you attribute specific prompts to OpenAI Agent LLM spans.
  Be sure to **pull the prompt** before logging it, otherwise the prompt will
  not be visible on Confident AI.
</Note>

## Evals Usage

### Online evals

If your LangChain application is in production, and you still want to run evaluations on your traces, use [online evals](/docs/llm-tracing/evaluations). It lets you run evaluations on all incoming traces on Confident AI's server.

<Steps>

<Step title="Create metric collection">

Create a metric collection on [Confident AI](https://app.confident.ai) with the metrics you wish to use to evaluate your LangGraph agent. Copy the name of the metric collection.

<Frame caption="Create metric collection">
  <video data-video="metrics.createCollection" controls autoPlay />
</Frame>

<Warning>
  The current LangChain integration supports metrics that only evaluate `Input`
  and `Actual Output` in addition to the **Task Completion** metric.
</Warning>

</Step>

<Step title="Run evals">

Set the `metric_collection` name to evaluate various components of your LangChain application.

<Tabs group="eval-method">

<Tab title="Agent Span">

This is the top level component of your LangChain application. Also a very idle component to evaluate with the **Task Completion** metric.

<CodeBlocks >

<CodeBlock language="python">

```python Python {23} maxLines=100
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

from deepeval.integrations.langchain import CallbackHandler

@tool
def multiply(a: int, b: int) -> int:
    """Returns the product of two numbers"""
    return a * b

llm = ChatOpenAI(model="gpt-4o-mini")
agent_prompt = ChatPromptTemplate.from_messages([("system", "You are a helpful assistant that can perform mathematical operations."), ("human", "{input}"), MessagesPlaceholder("agent_scratchpad")])
agent = create_tool_calling_agent(llm, [multiply], agent_prompt)
agent_executor = AgentExecutor(agent=agent, tools=[multiply], verbose=True)

result = agent_executor.invoke(
    {"input": "What is 8 multiplied by 6?"},
    config={
        "callbacks": [
          CallbackHandler(metric_collection="<metric_collection_name>")
        ]
    },
)
```

</CodeBlock>

<CodeBlock language="typescript">

```typescript TypeScript {12} maxLines=100
import { ChatOpenAI } from "@langchain/openai";
import {
  ChatPromptTemplate,
  MessagesPlaceholder,
} from "@langchain/core/prompts";
import { DynamicStructuredTool } from "@langchain/core/tools";
import { createToolCallingAgent, AgentExecutor } from "langchain/agents";

import { DeepEvalCallbackHandler } from "deepeval-ts/integrations";

const handler = new DeepEvalCallbackHandler({
  metricCollection: "<metric_collection_name>",
});

const multiplyTool = new DynamicStructuredTool({
  name: "multiply",
  description: "Returns the product of two numbers",
  schema: {
    a: { type: "number", description: "The first number" },
    b: { type: "number", description: "The second number" },
  },
  func: async ({ a, b }: { a: number; b: number }) => a * b,
});

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
});

const agentPrompt = ChatPromptTemplate.fromMessages([
  [
    "system",
    "You are a helpful assistant that can perform mathematical operations.",
  ],
  ["human", "{input}"],
  new MessagesPlaceholder("agent_scratchpad"),
]);

const agent = await createToolCallingAgent({
  llm,
  tools: [multiplyTool],
  prompt: agentPrompt,
});

const agentExecutor = new AgentExecutor({
  agent,
  tools: [multiplyTool],
  verbose: true,
});

const main = async () => {
  const result = await agentExecutor.invoke(
    { input: "What is 8 multiplied by 6?" },
    { callbacks: [handler] }
  );

  console.log(result);
};
```

</CodeBlock>

</CodeBlocks>

</Tab>

<Tab title="LLM Span">

For LLM spans, you can set the `metric_collection` or `metricCollection` name in the `metadata` parameter of the language model instance.

<CodeBlocks >

<CodeBlock language="python">

```python Python {15}
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from deepeval.integrations.langchain import CallbackHandler

@tool
def multiply(a: int, b: int) -> int:
    """Returns the product of two numbers"""
    return a * b

llm = ChatOpenAI(
  model="gpt-4o-mini",
  metadata={"metric_collection": "<metric_collection_name>"}
)

agent_prompt = ChatPromptTemplate.from_messages([("system", "You are a helpful assistant that can perform mathematical operations."), ("human", "{input}"), MessagesPlaceholder("agent_scratchpad")])
agent = create_tool_calling_agent(llm, [multiply], agent_prompt)
agent_executor = AgentExecutor(agent=agent, tools=[multiply], verbose=True)

result = agent_executor.invoke(
    {"input": "What is 8 multiplied by 6?"},
    config={
        "callbacks": [
          CallbackHandler(metric_collection="<metric_collection_name>")
        ]
    },
)
```

</CodeBlock>

<CodeBlock language="typescript">

```typescript TypeScript {25,26,27}
import { ChatOpenAI } from "@langchain/openai";
import {
  ChatPromptTemplate,
  MessagesPlaceholder,
} from "@langchain/core/prompts";
import { DynamicStructuredTool } from "@langchain/core/tools";
import { createToolCallingAgent, AgentExecutor } from "langchain/agents";

import { DeepEvalCallbackHandler } from "deepeval-ts/integrations";

const handler = new DeepEvalCallbackHandler({});

const multiplyTool = new DynamicStructuredTool({
  name: "multiply",
  description: "Returns the product of two numbers",
  schema: {
    a: { type: "number", description: "The first number" },
    b: { type: "number", description: "The second number" },
  },
  func: async ({ a, b }: { a: number; b: number }) => a * b,
});

const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
  metadata: {
    metricCollection: "<metric_collection_name>",
  },
});

const agentPrompt = ChatPromptTemplate.fromMessages([
  [
    "system",
    "You are a helpful assistant that can perform mathematical operations.",
  ],
  ["human", "{input}"],
  new MessagesPlaceholder("agent_scratchpad"),
]);

const agent = await createToolCallingAgent({
  llm,
  tools: [multiplyTool],
  prompt: agentPrompt,
});

const agentExecutor = new AgentExecutor({
  agent,
  tools: [multiplyTool],
  verbose: true,
});

const main = async () => {
  const result = await agentExecutor.invoke(
    { input: "What is 8 multiplied by 6?" },
    { callbacks: [handler] }
  );

  console.log(result);
};
```

</CodeBlock>

</CodeBlocks>

</Tab>

<Tab title="Tool Span">

For tool spans, you can set the `metric_collection` parameter of the DeepEval's `@tool` decorator.

<Note>
  If your application does not use `@tool` decorator, you can also use
  `@observe` decorator from `deepeval.tracing` to set the metric collection.
</Note>

<CodeBlocks >

<CodeBlock title="Using @tool">

```python {8,10}
# from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from deepeval.integrations.langchain import CallbackHandler

from deepeval.integrations.langchain import tool

@tool(metric_collection="test_collection_1")
def multiply(a: int, b: int) -> int:
    """Returns the product of two numbers"""
    return a * b

llm = ChatOpenAI(
  model="gpt-4o-mini",
  metadata={"metric_collection": "<metric_collection_name>"}
)

agent_prompt = ChatPromptTemplate.from_messages([("system", "You are a helpful assistant that can perform mathematical operations."), ("human", "{input}"), MessagesPlaceholder("agent_scratchpad")])
agent = create_tool_calling_agent(llm, [multiply], agent_prompt)
agent_executor = AgentExecutor(agent=agent, tools=[multiply], verbose=True)

result = agent_executor.invoke(
    {"input": "What is 8 multiplied by 6?"},
    config={
        "callbacks": [
          CallbackHandler(metric_collection="<metric_collection_name>")
        ]
    },
)
```

</CodeBlock>

<CodeBlock title="Using @observe">

```python
from deepeval.tracing import observe
from deepeval.integrations.langchain import CallbackHandler

@observe(type="tool", metric_collection="test_collection_1")
def multiply(a: int, b: int) -> int:
    """Returns the product of two numbers"""
    return a * b
```

</CodeBlock>

</CodeBlocks>

</Tab>

</Tabs>

<Success>
  All incoming traces will now be evaluated using metrics from your metric
  collection.
</Success>

</Step>
</Steps>

### End-to-end evals

Running [end-to-end evals](/docs/llm-evaluation/single-turn/end-to-end) on your LangChain agent evaluates your agent locally, and is the recommended approach if your agent is in a development or testing environment.

<Steps>

<Step title="Create metric">

```python
from deepeval.metrics import TaskCompletionMetric

task_completion = TaskCompletionMetric(
    threshold=0.7,
    model="gpt-4o-mini",
    include_reason=True
)
```

</Step>

<Warning>
  Similar to online evals, you can only run end-to-end evals on LangChain using
  `TaskCompletionMetric`.
</Warning>

<Step title="Run evals">

Provide your metrics to the `CallbackHandler`. Then, use the dataset's `evals_iterator` to invoke your LangChain agent for each golden.

<Tabs>
<Tab title="Synchronous">

```python Python focus={16-34} maxLines=34
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from deepeval.metrics import TaskCompletionMetric
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from deepeval.integrations.langchain import CallbackHandler

@tool
def multiply(a: int, b: int) -> int:
    """Returns the product of two numbers"""
    return a * b

agent_executor = AgentExecutor(agent=create_tool_calling_agent(ChatOpenAI(model="gpt-4o-mini"), [multiply], ChatPromptTemplate.from_messages([("system", "You are a helpful assistant that can perform mathematical operations."), ("human", "{input}"), MessagesPlaceholder("agent_scratchpad")])), tools=[multiply], verbose=True)
task_completion_metric = TaskCompletionMetric(threshold=0.7, model="gpt-4o-mini", include_reason=True)

from deepeval.dataset import EvaluationDataset, Golden

dataset = EvaluationDataset(
    goldens=[
        Golden(input="What is 3 * 12?"),
        Golden(input="What is 8 * 6?"),
    ]
)

def llm_agent_eval(golden: Golden):
    result = agent_executor.invoke({"input": golden.input},
        config={
            "callbacks": [CallbackHandler(metrics=[task_completion_metric])]
        },
    )
    return result

for golden in dataset.evals_iterator():
    llm_agent_eval(golden)
```

</Tab>
<Tab title="Asynchronous">

```python Python focus={1, 17-34} maxLines=34
import asyncio
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from deepeval.metrics import TaskCompletionMetric
from deepeval.integrations.langchain import CallbackHandler

@tool
def multiply(a: int, b: int) -> int:
    """Returns the product of two numbers"""
    return a * b

agent_executor = AgentExecutor(agent=create_tool_calling_agent(ChatOpenAI(model="gpt-4o-mini"), [multiply], ChatPromptTemplate.from_messages([("system", "You are a helpful assistant that can perform mathematical operations."), ("human", "{input}"), MessagesPlaceholder("agent_scratchpad")])), tools=[multiply], verbose=True)
task_completion_metric = TaskCompletionMetric(threshold=0.7, model="gpt-4o-mini", include_reason=True)

from deepeval.dataset import EvaluationDataset, Golden

dataset = EvaluationDataset(
    goldens=[
        Golden(input="What is 3 * 12?"),
        Golden(input="What is 8 * 6?"),
    ]
)

for golden in dataset.evals_iterator():
    task = asyncio.create_task(
        agent_executor.ainvoke({"input": golden.input},
            config={
                "callbacks": [CallbackHandler(metrics=[task_completion_metric])]
            },
        )
    )
    dataset.evaluate(task)
```

</Tab>
</Tabs>

<Success>
  This will automatically generate a test run with evaluated traces using inputs
  from your dataset.
</Success>

</Step>
</Steps>

### View on Confident AI

You can view the evals on [Confident AI](https://app.confident.ai) by clicking on the link in the output printed in the console.

<Frame>
  <video data-video="evaluation.singleTurnE2E" controls autoPlay playsInline />
</Frame>
