---
slug: integrations/third-party/openai
subtitle: Use Confident AI for LLM observability and evals for OpenAI
---

## Overview

Confident AI lets you trace and evaluate OpenAI calls, whether standalone or used as a component within a larger application.

## Tracing Quickstart

<Steps>

<Step title="Install Dependencies">

Run the following command to install the required packages:

```bash
pip install -U deepeval openai
```

</Step>

<Step title="Setup Confident AI Key">

Login to Confident AI using your Confident API key.

<CodeBlocks>

<CodeBlock language="bash">

```bash
deepeval login
```

</CodeBlock>

<CodeBlock language="python">

```python
import deepeval

deepeval.login("<your-confident-api-key>")

```

</CodeBlock>

</CodeBlocks>

</Step>

<Step title="Configure OpenAI">

To begin tracing your OpenAI calls as a component in your application, import OpenAI from DeepEval instead.

<Tabs>
  <Tab title="Chat Completions">

```python main.py {1}
from deepeval.openai import OpenAI

client = OpenAI()

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the weather in France?"},
    ],
)
```

</Tab>
<Tab title="Responses">

```python main.py {1}
from deepeval.openai import OpenAI

client = OpenAI()

response = client.responses.create(
    model="gpt-4o-mini",
    instructions="You are a helpful assistant.",
    input="What is the weather in France?",
)
```

</Tab>
<Tab title="Async Chat Completions">

```python main.py {2}
import asyncio
from deepeval.openai import AsyncOpenAI

client = AsyncOpenAI()

async def generate_response(input: str) -> str:
    response = await client.chat.completions.create(
        model="gpt-4.1",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": input},
        ],
    )
    return response

response = asyncio.run(generate_response("What is the weather in France?"))
```

</Tab>
<Tab title="Async Responses">

```python main.py {2}
import asyncio
from deepeval.openai import AsyncOpenAI

client = AsyncOpenAI()

async def generate_response(input: str) -> str:
    response = await client.responses.create(
        model="gpt-4o-mini",
        instructions="You are a helpful assistant.",
        input=input,
    )
    return response

response = asyncio.run(generate_response("What is the weather in France?"))
```

</Tab>
</Tabs>

<Note>
  DeepEval's OpenAI client traces `chat.completions.create`, `beta.chat.completions.parse`, and `responses.create` methods.
</Note>
</Step>

<Step title="Run OpenAI">

Invoke your agent by executing the script:

```bash
python main.py
```

You can directly view the traces on [Confident AI](https://app.confident.ai) by clicking on the link in the output printed in the console.

</Step>

</Steps>

## Advanced Usage

### Logging prompts

If you are [managing prompts](/docs/llm-evaluation/prompt-optimization/prompt-versioning) on Confident AI and wish to log them, pass your `Prompt` object to the `trace` context using `LlmSpanContext`.

```python main.py
from deepeval.openai import OpenAI
from deepeval.prompt import Prompt
from deepeval.tracing import trace, LlmSpanContext

prompt = Prompt(alias="my-prompt")
prompt.pull(version="00.00.01")

client = OpenAI()

with trace(LlmSpanContext(prompt=prompt)):
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": prompt.interpolate(name="Jhon")}, # should be a string system prompt
            {"role": "user", "content": "Hello, how are you?"},
        ],
    )
```

### Logging threads

Threads are used to group related traces together, and are useful for chat apps, agents, or any multi-turn interactions. Learn more about [threads](/docs/llm-tracing/advanced-features/threads) here. You can set the `thread_id` in the `trace` context.

```python main.py
from deepeval.openai import OpenAI
from deepeval.tracing import trace

client = OpenAI()

with trace(thread_id="test_thread_id_1"):
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Hello, how are you?"},
        ],
    )
```

### Other trace attributes

Confident AI's LLM tracing advanced features provide teams with the ability to set certain attributes for each trace when invoking your OpenAI client.

For example, `user_id` can be used to enable user analytics. You can learn more about [user id](/docs/llm-tracing/advanced-features/users) here. Similarly, you can set the `metadata` to attach any metadata to the trace.

You can set these attributes in the `trace` context when invoking your OpenAI client.

```python main.py maxLines=100
from deepeval.openai import OpenAI
from deepeval.tracing import trace

client = OpenAI()

with trace(
    thread_id="test_thread_id_1",
    metadata={"test_metadata_1": "test_metadata_1"},
):
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Hello, how are you?"},
        ],
    )
```
<Note>
  This override the trace attributes that were set using `update_current_trace` method.
</Note>

<Accordion title='View Trace Attributes'>
<ParamField path="name" type="str" required={false}>
  The name of the trace. [Learn more](/docs/llm-tracing/advanced-features/name).
</ParamField>

<ParamField path="tags" type="List[str]" required={false}>
  Tags are string labels that help you group related traces. [Learn
  more](/docs/llm-tracing/advanced-features/tags).
</ParamField>

<ParamField path="metadata" type="Dict" required={false}>
  Attach any metadata to the trace. [Learn
  more](/docs/llm-tracing/advanced-features/metadata).
</ParamField>

<ParamField path="thread_id" type="str" required={false}>
  Supply the thread or conversation ID to view and evaluate conversations.
  [Learn more](/docs/llm-tracing/advanced-features/threads).
</ParamField>

<ParamField path="user_id" type="str" required={false}>
  Supply the user ID to enable user analytics. [Learn
  more](/docs/llm-tracing/advanced-features/users).
</ParamField>

<Info>
  Each attribute is **optional**, and works the same way as the [native tracing
  features](/docs/llm-tracing/introduction) on Confident AI.
</Info>
</Accordion>

## Evals Usage

### Online evals
If your OpenAI application is in production, and you still want to run evaluations on your traces, use [online evals](/docs/llm-tracing/evaluations). It lets you run evaluations on all incoming traces on Confident AI's server.

<Steps>

<Step title="Create metric collection">

Create a metric collection on [Confident AI](https://app.confident.ai) with the metrics you wish to use to evaluate your OpenAI agent. Copy the name of the metric collection.

<Frame caption="Create metric collection">
  <video
    src="https://confident-docs.s3.us-east-1.amazonaws.com/integrations%3Athird-party-integrations%3Acreate-metric-collection-4k.mp4"
    controls
    autoPlay
  />
</Frame>

</Step>

<Step title="Run evals">

Set the `metric_collection` name in the `trace` context using `LlmSpanContext` when invoking your OpenAI client to evaluate Llm Spans.

```python main.py
from deepeval.openai import OpenAI
from deepeval.tracing import trace, LlmSpanContext

client = OpenAI()

with trace(
    LlmSpanContext(metric_collection="test_collection_1")
):
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Hello, how are you?"},
        ],
    )
```

</Step>

</Steps>


### End-to-end evals

Confident AI allows you to run [end-to-end evals](/llm-evaluation/single-turn/end-to-end) on your OpenAI client to evaluate your OpenAI calls directly. This is recommended if you are testing your OpenAI calls in isolation.

<Steps>

<Step title="Create metric">

```python
from deepeval.metrics import AnswerRelevancyMetric

task_completion = AnswerRelevancyMetric(
    threshold=0.7,
    model="gpt-4o-mini",
    include_reason=True
)
```

<Warning>
  You can only run end-to-end evals on OpenAI using metrics that evaluate
  `input`, `output`, or `tools_called`. You can pass parameters like `expected_output`, `expected_tools`, `context` and `retrieval_context` to the `trace` context.
</Warning>

</Step>

<Step title="Run evals">

Replace your `OpenAI` client with DeepEval's. Then, use the dataset's `evals_iterator` to invoke your OpenAI client for each golden.

<Tabs>
<Tab title="Chat Completions">

```python main.py
from deepeval.openai import OpenAI
from deepeval.metrics import AnswerRelevancyMetric, BiasMetric
from deepeval.dataset import EvaluationDataset
from deepeval.tracing import trace

client = OpenAI()

dataset = EvaluationDataset()
dataset.pull("your-dataset-alias")

for golden in dataset.evals_iterator():
    # run OpenAI client
    with trace(
        LlmSpanContext(
            metrics=[AnswerRelevancyMetric(), BiasMetric()],
            expected_output=golden.expected_output
        ),
    ):
        client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": golden.input}
            ],
        )
```

</Tab>
<Tab title="Responses">

```python main.py
from deepeval.openai import OpenAI
from deepeval.metrics import AnswerRelevancyMetric, BiasMetric
from deepeval.dataset import EvaluationDataset, Golden
from deepeval.tracing import trace, LlmSpanContext

client = OpenAI()

dataset = EvaluationDataset()
dataset.pull("your-dataset-alias")

for golden in dataset.evals_iterator():
    # run OpenAI client
    with trace(
        LlmSpanContext(
            metrics=[AnswerRelevancyMetric(), BiasMetric()],
            expected_output=golden.expected_output
        ),
    ):
        client.responses.create(
            model="gpt-4o",
            instructions="You are a helpful assistant.",
            input=golden.input,
        )
```

</Tab>
<Tab title="Async Chat Completions">

```python main.py
import asyncio
from deepeval.openai import AsyncOpenAI
from deepeval.metrics import AnswerRelevancyMetric, BiasMetric
from deepeval.dataset import EvaluationDataset
from deepeval.tracing import trace, LlmSpanContext

async_client = AsyncOpenAI()

async def openai_llm_call(input):
    with trace(
        LlmSpanContext(
            metrics=[AnswerRelevancyMetric(), BiasMetric()],
            expected_output=golden.expected_output
        ),
    ):
        return await async_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are a helpful chatbot. Always generate a string response."},
                {"role": "user", "content": input},
            ],
        )

dataset = EvaluationDataset()
dataset.pull("your-dataset-alias")

for golden in dataset.evals_iterator():
    task = asyncio.create_task(openai_llm_call(golden.input))
    dataset.evaluate(task)
```

</Tab>
<Tab title="Async Responses">

```python main.py
import asyncio
from deepeval.openai import AsyncOpenAI
from deepeval.metrics import AnswerRelevancyMetric, BiasMetric
from deepeval.dataset import EvaluationDataset, Golden
from deepeval.tracing import trace, LlmSpanContext

async_client = AsyncOpenAI()

async def openai_llm_call(input):
    with trace(
        LlmSpanContext(
            metrics=[AnswerRelevancyMetric(), BiasMetric()],
            expected_output=golden.expected_output
        ),
    ):
        return await async_client.responses.create(
            model="gpt-4o",
            instructions="You are a helpful assistant.",
            input=input,  
        )


dataset = EvaluationDataset()
dataset.pull("your-dataset-alias")

for golden in dataset.evals_iterator():
    task = asyncio.create_task(openai_llm_call(golden.input))
    dataset.evaluate(task)
```

</Tab>
</Tabs>

<Success>
  This will automatically generate a test run with evaluated OpenAI traces using
  inputs from your dataset.
</Success>

</Step>
</Steps>

### Using OpenAI in component-level evals

You can also evaluate OpenAI calls through component-level evals. This approach is recommended if you are testing your OpenAI calls as a component in a larger application system.

<Steps>

<Step title="Create metric">

```python
from deepeval.metrics import AnswerRelevancyMetric

task_completion = AnswerRelevancyMetric(
    threshold=0.7,
    model="gpt-4o-mini",
    include_reason=True
)
```

<Warning>
  As with end-to-end evals, you can only use metrics that evaluate `input`,
  `output`, or `tools_called`.
</Warning>

</Step>

<Step title="Run evals">

Replace your `OpenAI` client with DeepEval's. Then, use the dataset's `evals_iterator` to invoke your LLM application for each golden.

<Note>
  Make sure that each function or method in your LLM application is decorated
  with `@observe`.
</Note>

<Tabs>
  <Tab title="Chat Completions">

```python
from deepeval.openai import OpenAI
from deepeval.tracing import observe, trace, LlmSpanContext
from deepeval.dataset import EvaluationDataset
from deepeval.metrics import AnswerRelevancyMetric

client = OpenAI()

@observe()
def generate_response(input: str) -> str:
    with trace(
        LlmSpanContext(
            metrics=[AnswerRelevancyMetric()],
            expected_output=golden.expected_output
        ),
    ):
        response = client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": input},
            ],
        )
        return response

# Create dataset
dataset = EvaluationDataset()
dataset.pull("your-dataset-alias")

# Run component-level evaluation
for golden in dataset.evals_iterator():
    generate_response(golden.input)
```

</Tab>
<Tab title="Responses">

```python
from deepeval.openai import OpenAI
from deepeval.tracing import observe, trace, LlmSpanContext
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.dataset import EvaluationDataset

client = OpenAI()

@observe()
def generate_response(input: str) -> str:
    with trace(
        LlmSpanContext(
            metrics=[AnswerRelevancyMetric()],
            expected_output=golden.expected_output
        )
    ):
        response = client.responses.create(
            model="gpt-4.1",
            instructions="You are a helpful assistant.",
            input=input,
        )
        return response

# Create dataset
dataset = EvaluationDataset()
dataset.pull("your-dataset-alias")

# Run component-level evaluation
for golden in dataset.evals_iterator():
    generate_response(golden.input)
```

</Tab>
<Tab title="Async Chat Completions">

```python
from deepeval.openai import OpenAI
from deepeval.tracing import observe, trace, LlmSpanContext
from deepeval.dataset import EvaluationDataset
from deepeval.metrics import AnswerRelevancyMetric

client = OpenAI()

@observe()
def generate_response(input: str) -> str:
    with trace(
        LlmSpanContext(
            metrics=[AnswerRelevancyMetric()],
            expected_output=golden.expected_output
        ),
    ):
        response = client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": input},
            ],
        )
        return response


# Create dataset
dataset = EvaluationDataset()
dataset.pull("your-dataset-alias")

# Run component-level evaluation
for golden in dataset.evals_iterator():
    generate_response(golden.input)
```

</Tab>
<Tab title="Async Responses">

```python
import asyncio
from deepeval.openai import AsyncOpenAI
from deepeval.tracing import observe, trace, LlmSpanContext
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.dataset import EvaluationDataset

client = AsyncOpenAI()

@observe()
async def generate_response(input: str) -> str:
    with trace(
        LlmSpanContext(
            metrics=[AnswerRelevancyMetric()],
            expected_output=golden.expected_output
        )
    ):
        response = await client.responses.create(
            model="gpt-4.1",
            instructions="You are a helpful assistant.",
            input=input,
        )
        return response

# Create dataset
dataset = EvaluationDataset()
dataset.pull("your-dataset-alias")

# Run component-level evaluation
for golden in dataset.evals_iterator():
    task = asyncio.create_task(generate_response(golden.input))
    dataset.evaluate(task)
```

</Tab>
</Tabs>

</Step>
</Steps>
