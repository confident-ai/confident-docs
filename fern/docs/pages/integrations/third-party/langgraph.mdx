---
slug: integrations/third-party/langgraph
subtitle: Use Confident AI for LLM observability and evals for LangGraph
---

## Overview

[LangGraph](https://www.langchain.com/langgraph) is a framework for building reactive, multi-agent systems. Confident AI provides a `CallbackHandler` to trace and evaluate LangGraph agents.

## Tracing Quickstart

<Steps>

<Step title="Install Dependencies">

Run the following command to install the required packages:

```bash
pip install -U deepeval langgraph langchain langchain-openai
```

</Step>

<Step title="Setup Confident AI Key">

Login to Confident AI using your Confident API key.

<CodeBlocks>

<CodeBlock language="bash">

```bash
export CONFIDENT_API_KEY="<your-confident-api-key>"
```

</CodeBlock>

<CodeBlock language="python">

```python
import deepeval

deepeval.login("<your-confident-api-key>")

```

</CodeBlock>

</CodeBlocks>

</Step>

<Step title="Configure LangGraph">

Provide DeepEval's `CallbackHandler` to your LangGraph agent's invoke method.

```python main.py {3,19} maxLines=25
from langgraph.prebuilt import create_react_agent

from deepeval.integrations.langchain import CallbackHandler

def get_weather(city: str) -> str:
    """Returns the weather in a city"""
    return f"It's always sunny in {city}!"

agent = create_react_agent(
    model="openai:gpt-4o-mini",
    tools=[get_weather],
    prompt="You are a helpful assistant",
)

result = agent.invoke(
    input={
        "messages": [{"role": "user", "content": "what is the weather in sf"}]
    },
    config={"callbacks": [CallbackHandler()]},
)

print(result)
```

<Note>
  DeepEval's `CallbackHandler` extends LangChain's [`BaseCallbackHandler`](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html).
</Note>
</Step>

<Step title="Run LangGraph">

Invoke your agent by executing the script:

```bash
python main.py
```

You can directly view the traces on [Confident AI](https://app.confident.ai) by clicking on the link in the output printed in the console.

</Step>

</Steps>

### Trace attributes

You can also set custom trace attributes in your `CallbackHandler` for each agent invocation:

```python main.py focus={11-17} maxLines=20
from langgraph.prebuilt import create_react_agent
from deepeval.integrations.langchain import CallbackHandler

def get_weather(city: str) -> str:
    """Returns the weather in a city"""
    return f"It's always sunny in {city}!"

agent = create_react_agent(model="openai:gpt-4o-mini", tools=[get_weather], prompt="You are a helpful assistant")

result = agent.invoke(input={"messages": [{"role": "user", "content": "what is the weather in sf"}]},
    config={"callbacks": [CallbackHandler(
      name="Name of Trace",
      tags=["Tag 1", "Tag 2"],
      metadata={"Key": "Value"},
      thread_id="your-thread-id",
      user_id="your-user-id",
    )]},
)

print(result)
```

<Accordion title='View Trace Attributes'>
<ParamField path="name" type="str" required={false}>
  The name of the trace. [Learn more](/docs/llm-tracing/advanced-features/name).
</ParamField>

<ParamField path="tags" type="List[str]" required={false}>
  Tags are string labels that help you group related traces. [Learn
  more](/docs/llm-tracing/advanced-features/tags).
</ParamField>

<ParamField path="metadata" type="Dict" required={false}>
  Attach any metadata to the trace. [Learn
  more](/docs/llm-tracing/advanced-features/metadata).
</ParamField>

<ParamField path="thread_id" type="str" required={false}>
  Supply the thread or conversation ID to view and evaluate conversations.
  [Learn more](/docs/llm-tracing/advanced-features/threads).
</ParamField>

<ParamField path="user_id" type="str" required={false}>
  Supply the user ID to enable user analytics. [Learn
  more](/docs/llm-tracing/advanced-features/users).
</ParamField>

<Info>
  Each attribute is **optional**, and works the same way as the [native tracing
  features](/docs/llm-tracing/introduction) on Confident AI.
</Info>
</Accordion>

## Evals Usage

### Online evals

You can run [online evals](/docs/llm-tracing/evaluations) on your LangGraph agent, which will run evaluations on all incoming traces on Confident AI's servers. This approach is recommended if your agent is in production.

<Steps>

<Step title="Create metric collection">

Create a metric collection on [Confident AI](https://app.confident.ai) with the metrics you wish to use to evaluate your LangGraph agent.

<Frame caption="Create metric collection">
  <video
    src="https://confident-docs.s3.us-east-1.amazonaws.com/integrations%3Athird-party-integrations%3Acreate-metric-collection-4k.mp4"
    autoPlay
    controls
  />
</Frame>

<Warning>
  Your metric collection should only contain the **task completion metric**,
  which is the only supported metric for LangGraph.
</Warning>

</Step>

<Step title="Run evals">

Provide your metric collection name to `CallbackHandler`.

```python main.py focus={12-14} maxLines=18
from langgraph.prebuilt import create_react_agent
from deepeval.integrations.langchain import CallbackHandler

def get_weather(city: str) -> str:
    """Returns the weather in a city"""
    return f"It's always sunny in {city}!"

agent = create_react_agent(model="openai:gpt-4o-mini", tools=[get_weather], prompt="You are a helpful assistant")

result = agent.invoke(input={"messages": [{"role": "user", "content": "what is the weather in sf"}]},
    config={
        "callbacks": [CallbackHandler(
          metric_collection="<metric-collection-with-task-completion>"
        )]
    },
)

print(result)
```

<Success>
  All incoming traces will now be evaluated using metrics from your metric
  collection.
</Success>

</Step>
</Steps>

### End-to-end evals

Running [end-to-end evals](/docs/llm-evaluation/single-turn/end-to-end) on your LangGraph agent evaluates your agent locally, and is the recommended approach if your agent is in a development or testing environment.

<Steps>

<Step title="Create metric">

```python
from deepeval.metrics import TaskCompletionMetric

task_completion = TaskCompletionMetric(
    threshold=0.7,
    model="gpt-4o-mini",
    include_reason=True
)
```

</Step>

<Warning>
  Similar to online evals, you can only run end-to-end evals on LangGraph using
  `TaskCompletionMetric`.
</Warning>

<Step title="Run evals">

Provide your metrics to the `CallbackHandler`. Then, use the dataset's `evals_iterator` to invoke your LangGraph agent for each golden.

<Tabs>
<Tab title="Synchronous">

```python main.py focus={12-25} maxLines=25
from langgraph.prebuilt import create_react_agent
from deepeval.metrics import TaskCompletionMetric
from deepeval.integrations.langchain import CallbackHandler

def get_weather(city: str) -> str:
    """Returns the weather in a city"""
    return f"It's always sunny in {city}!"

agent = create_react_agent(model="openai:gpt-4o-mini",tools=[get_weather],prompt="You are a helpful assistant",)
task_completion = TaskCompletionMetric(threshold=0.7, model="gpt-4o-mini", include_reason=True)

from deepeval.dataset import Golden, EvaluationDataset

goldens = [
    Golden(input="What is the weather in Bogotá, Colombia?"),
    Golden(input="What is the weather in Paris, France?"),
]

dataset = EvaluationDataset(goldens=goldens)

for golden in dataset.evals_iterator():
    agent.invoke(
        input={"messages": [{"role": "user", "content": golden.input}]},
        config={"callbacks": [CallbackHandler(metrics=[task_completion])]},
    )
```

</Tab>
<Tab title="Asynchronous">

```python main.py focus={1, 13-29} maxLines=29
import asyncio
from langgraph.prebuilt import create_react_agent
from deepeval.metrics import TaskCompletionMetric
from deepeval.integrations.langchain import CallbackHandler

def get_weather(city: str) -> str:
    """Returns the weather in a city"""
    return f"It's always sunny in {city}!"

agent = create_react_agent(model="openai:gpt-4o-mini",tools=[get_weather],prompt="You are a helpful assistant",)
task_completion = TaskCompletionMetric(threshold=0.7, model="gpt-4o-mini", include_reason=True)

from deepeval.dataset import Golden, EvaluationDataset

goldens = [
    Golden(input="What is the weather in Bogotá, Colombia?"),
    Golden(input="What is the weather in Paris, France?"),
]

dataset = EvaluationDataset(goldens=goldens)

for golden in dataset.evals_iterator():
    task = asyncio.create_task(
        agent.ainvoke(
            input={"messages": [{"role": "user", "content": golden.input}]},
            config={"callbacks": [CallbackHandler(metrics=[task_completion])]},
        )
    )
    dataset.evaluate(task)
```

</Tab>
</Tabs>

<Success>
  This will automatically generate a test run with evaluated traces using inputs
  from your dataset.
</Success>

</Step>
</Steps>

### View on Confident AI

You can view the evals on [Confident AI](https://app.confident.ai) by clicking on the link in the output printed in the console.

<Frame>
  <video
    src="https://confident-bucket.s3.us-east-1.amazonaws.com/end-to-end%3Alanggraph.mp4"
    controls
    autoPlay
    playsInline
  />
</Frame>
