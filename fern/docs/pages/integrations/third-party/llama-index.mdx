---
slug: integrations/third-party/llama-index
subtitle: Use Confident AI for LLM observability and evals for LlamaIndex
---

## Overview

[LlamaIndex](https://www.llamaindex.ai/) is an LLM framework that makes it easy to build knowledge agents from complex data. Confident AI allows you to trace and evaluate LlamaIndex agents in just a few lines of code.

## Tracing Quickstart

<Steps>

<Step title="Install Dependencies">

Run the following command to install the required packages:

```bash
pip install -U deepeval llama-index
```

</Step>

<Step title="Setup Confident AI Key">

Login to Confident AI using your Confident API key.

<CodeBlocks>

<CodeBlock language="bash">

```bash
deepeval login
```

</CodeBlock>

<CodeBlock language="python">

```python
import deepeval

deepeval.login("<your-confident-api-key>")

```

</CodeBlock>

</CodeBlocks>

</Step>

<Step title="Configure LlamaIndex">

Instrument LlamaIndex using `instrument_llama_index` to enable Confident AI's `LlamaIndexHandler`.

```python main.py maxLines=100 {6,7}
import asyncio
from llama_index.llms.openai import OpenAI
from llama_index.core.agent import FunctionAgent
import llama_index.core.instrumentation as instrument

from deepeval.integrations.llama_index import instrument_llama_index
instrument_llama_index(instrument.get_dispatcher())

def multiply(a: float, b: float) -> float:
    """Useful for multiplying two numbers."""
    return a * b

agent = FunctionAgent(
    tools=[multiply],
    llm=OpenAI(model="gpt-4o-mini"),
    system_prompt="You are a helpful assistant that can perform calculations.",
)

async def llm_app(input: str):
    return await agent.run(input)

asyncio.run(llm_app("What is 3 * 12?"))
```

<Note>
  Now whenever you use LlamaIndex, DeepEval will collect LlamaIndex traces and
  publish them to Confident AI.
</Note>

You can directly view the traces on [Confident AI](https://app.confident.ai) by clicking on the link in the output printed in the console.

</Step>

</Steps>

## Advanced Usage

### Logging threads

Threads are used to group related traces together, and are useful for chat apps, agents, or any multi-turn interactions. You can learn more about [threads](/docs/llm-tracing/advanced-features/threads) here. You can set the `thread_id` in the `trace` context.

```python main.py
from deepeval.tracing import trace
...
async def llm_app(input: str):
    with trace(thread_id="test_thread_id_1"):
      return await agent.run(input)
```

### Logging metadata

You can also set the `metadata` in the `trace` context.

```python main.py
from deepeval.tracing import trace
...
async def llm_app(input: str):
    with trace(metadata={"key": "value"}):
      return await agent.run(input)
```

`trace` also allows you to set the `name`, `tags` and `user_id` in the `trace` context.

<Accordion title='View Trace Attributes'>
<ParamField path="name" type="str" required={false}>
  The name of the trace. [Learn more](/docs/llm-tracing/advanced-features/name).
</ParamField>

<ParamField path="tags" type="List[str]" required={false}>
  Tags are string labels that help you group related traces. [Learn
  more](/docs/llm-tracing/advanced-features/tags).
</ParamField>

<ParamField path="metadata" type="Dict" required={false}>
  Attach any metadata to the trace. [Learn
  more](/docs/llm-tracing/advanced-features/metadata).
</ParamField>

<ParamField path="thread_id" type="str" required={false}>
  Supply the thread or conversation ID to view and evaluate conversations.
  [Learn more](/docs/llm-tracing/advanced-features/threads).
</ParamField>

<ParamField path="user_id" type="str" required={false}>
  Supply the user ID to enable user analytics. [Learn
  more](/docs/llm-tracing/advanced-features/users).
</ParamField>

<Info>
  Each attribute is **optional**, and works the same way as the [native tracing
  features](/docs/llm-tracing/introduction) on Confident AI.
</Info>
</Accordion>


### Logging prompts

If you are [managing prompts](/docs/llm-evaluation/prompt-optimization/prompt-versioning) on Confident AI and wish to log them in the Llm Span, pass your `Prompt` object to the `LlmSpanContext` in the `trace` context.

```python main.py
from deepeval.prompt import Prompt
from deepeval.tracing.trace_context import LlmSpanContext

prompt = Prompt(alias="<prompt-alias>")
prompt.pull(version="00.00.01")

...

async def llm_app(input: str):
    with trace(llm_span_context=LlmSpanContext(prompt=prompt)):
      return await agent.run(input)

```


## Evals Usage

### Online evals

You can run [online evals](/docs/llm-tracing/evaluations) on your LlamaIndex agent, which will run evaluations on all incoming traces on Confident AIâ€™s servers. This approach is recommended if your agent is in production.

<Steps>

<Step title="Create metric collection">

Create a metric collection on [Confident AI](https://app.confident.ai) with the metrics you wish to use to evaluate your LlamaIndex agent.

<Frame caption="Create metric collection">
  <video
    src="https://confident-docs.s3.us-east-1.amazonaws.com/integrations%3Athird-party-integrations%3Acreate-metric-collection-4k.mp4"
    autoPlay
    controls
  />
</Frame>

<Warning>
  Your metric collection should only contain metrics that don't require
  `retrieval_context`, `context`, `expected_output`, or `expected_tools` for
  evaluation.
</Warning>
</Step>

<Step title="Run evals">

Confident AI supports [online evals](/docs/llm-tracing/evaluations) for LlamaIndex's agent and llm spans. Set the `metric_collection` name in the `trace` context when invoking your LlamaIndex agent.

<Tabs>

<Tab title="Agent Span">
```python main.py
from llama_index.llms.openai import OpenAI
from llama_index.core.agent import FunctionAgent

from deepeval.tracing import trace
from deepeval.tracing.trace_context import AgentSpanContext

from deepeval.integrations.llama_index import instrument_llama_index
import llama_index.core.instrumentation as instrument
from deepeval.metrics import TaskCompletionMetric

instrument_llama_index(instrument.get_dispatcher())

def multiply(a: float, b: float) -> float:
    """Useful for multiplying two numbers."""
    return a * b


agent = FunctionAgent(
    tools=[multiply],
    llm=OpenAI(model="gpt-4o-mini"),
    system_prompt="You are a helpful assistant that can perform calculations.",
)


async def llm_app(input: str):
    agent_span_context = AgentSpanContext(
        metrics=[TaskCompletionMetric()],
    )
    with trace(agent_span_context=agent_span_context):
        await agent.run(input)
```
</Tab>

<Tab title="Llm Span">
```python main.py
from llama_index.llms.openai import OpenAI
from llama_index.core.agent import FunctionAgent

from deepeval.tracing import trace
from deepeval.tracing.trace_context import LlmSpanContext

from deepeval.integrations.llama_index import instrument_llama_index
import llama_index.core.instrumentation as instrument
from deepeval.metrics import AnswerRelevancyMetric

instrument_llama_index(instrument.get_dispatcher())

def multiply(a: float, b: float) -> float:
    """Useful for multiplying two numbers."""
    return a * b


agent = FunctionAgent(
    tools=[multiply],
    llm=OpenAI(model="gpt-4o-mini"),
    system_prompt="You are a helpful assistant that can perform calculations.",
)


async def llm_app(input: str):
    llm_span_context = LlmSpanContext(
        metrics=[AnswerRelevancyMetric()],
    )
    with trace(llm_span_context=llm_span_context):
        await agent.run(input)
```
</Tab>

</Tabs>

<Success>
  All incoming traces will now be evaluated using metrics from your metric
  collection.
</Success>

</Step>
</Steps>

### End-to-end evals

Running [end-to-end evals](/docs/llm-evaluation/single-turn/end-to-end) on your LlamaIndex agent evaluates your agent locally, and is the recommended approach if your agent is in a development or testing environment.

<Steps>

<Step title="Create metric">

```python
from deepeval.metrics import AnswerRelevancyMetric

answer_relevancy_metric = AnswerRelevancyMetric(
    threshold=0.7,
    model="gpt-4o-mini",
    include_reason=True
)
```

</Step>

<Warning>
  Similar to online evals, you can only run end-to-end evals on metrics that don't require `retrieval_context`, `context`, `expected_output`, or `expected_tools` for evaluation.
</Warning>

<Step title="Run evals">

Provide your metrics. Then, use the dataset's `evals_iterator` to invoke your LlamaIndex agent for each golden.

<Tabs>
<Tab title="Asynchronous">

```python main.py maxLines=100
import asyncio

from llama_index.llms.openai import OpenAI
from llama_index.core.agent import FunctionAgent
import llama_index.core.instrumentation as instrument

from deepeval.integrations.llama_index import instrument_llama_index
from deepeval.tracing.trace_context import AgentSpanContext
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.tracing import trace

instrument_llama_index(instrument.get_dispatcher())


def multiply(a: float, b: float) -> float:
    """Useful for multiplying two numbers."""
    return a * b

agent = FunctionAgent(
    tools=[multiply],
    llm=OpenAI(model="gpt-4o-mini"),
    system_prompt="You are a helpful assistant that can perform calculations.",
)

answer_relevancy_metric = AnswerRelevancyMetric()

async def llm_app(input: str):
    agent_span_context = AgentSpanContext(
        metrics=[answer_relevancy_metric],
    )
    with trace(agent_span_context=agent_span_context):
        return await agent.run(input)
```

</Tab>
</Tabs>

<Success>
  This will automatically generate a test run with evaluated traces using inputs
  from your dataset.
</Success>

</Step>
</Steps>

### View on Confident AI

You can view the evals on [Confident AI](https://app.confident.ai) by clicking on the link in the output printed in the console.

<Frame>
<video
  src="https://confident-bucket.s3.us-east-1.amazonaws.com/end-to-end%3Allama-index-1080.mp4"
  controls
  autoPlay
/>
</Frame>
