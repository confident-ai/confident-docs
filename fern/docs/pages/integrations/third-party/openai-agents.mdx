---
slug: integrations/third-party/openai-agents
subtitle: Use Confident AI for LLM observability and evals for OpenAI Agents
---

## Overview

[OpenAI Agents](https://github.com/openai/openai-agents-python) is a lightweight framework for creating agentic workflows using agent swarms, handoffs, and tool use. Confident AI also allows you to trace OpenAI Agent workflows with one line of code.

## Tracing Quickstart

<Steps>

<Step title="Install Dependencies">

Run the following command to install the required packages:

```bash
pip install -U deepeval openai-agents
```

</Step>

<Step title="Setup Confident AI Key">

Login to Confident AI using your Confident API key.

<CodeBlocks>

<CodeBlock language="bash">

```bash
deepeval login
```

</CodeBlock>

<CodeBlock language="python">

```python
import deepeval

deepeval.login("<your-confident-api-key>")

```

</CodeBlock>

</CodeBlocks>

</Step>

<Step title="Configure OpenAI Agents">

Add DeepEval's trace processor to OpenAI Agents.

```python main.py {4}
from deepeval.openai_agents import DeepEvalTracingProcessor
from agents import add_trace_processor, Agent, Runner

add_trace_processor(DeepEvalTracingProcessor())

agent = Agent(name="Assistant", instructions="You are a helpful assistant")
result = Runner.run_sync(agent, "Write a haiku about recursion in programming.")
```

<Note>
Now whenever you use OpenAI Agents, DeepEval will collect OpenAI Agents traces and publish them to Confident AI.
</Note>
</Step>

<Step title="Run OpenAI Agents">

Invoke your agent by executing the script:

```bash
python main.py
```

You can directly view the traces on [Confident AI](https://app.confident.ai) by clicking on the link in the output printed in the console.

</Step>

</Steps>

## Advanced Usage

### Logging threads

Threads are used to group related traces together, and are useful for chat apps, agents, or any multi-turn interactions. You can learn more about [threads](/docs/llm-tracing/advanced-features/threads) here. You can set the `thread_id` in as `group_id` in the `trace` context.

```python main.py {7}
from deepeval.openai_agents import DeepEvalTracingProcessor
from agents import add_trace_processor, Agent, Runner, trace

add_trace_processor(DeepEvalTracingProcessor())
agent = Agent(name="Assistant", instructions="You are a helpful assistant")

with trace(workflow_name="test_workflow_1", group_id="test_group_id_1"):
      result = Runner.run_sync(agent, "Write a haiku about recursion in programming.")
```

### Logging metadata

You can also set the `metadata` in the `trace` context.

```python main.py {7}
from deepeval.openai_agents import DeepEvalTracingProcessor
from agents import add_trace_processor, Agent, Runner, trace

add_trace_processor(DeepEvalTracingProcessor())
agent = Agent(name="Assistant", instructions="You are a helpful assistant")

with trace(workflow_name="test_workflow_1", metadata={"test_metadata_1": "test_metadata_1"}):
      result = Runner.run_sync(agent, "Write a haiku about recursion in programming.")
```

### Streaming responses

Confident AI handles both asynchronous workflows and streamed responses. The following example shows how to trace streamed responses with OpenAI Agents.

```python main.py
from deepeval.openai_agents import DeepEvalTracingProcessor
from agents import add_trace_processor, Agent, Runner
import asyncio

add_trace_processor(DeepEvalTracingProcessor())

weather_agent = Agent(
    name="Weather Agent",
    instructions="You are a weather agent. You are given a question about the weather and you need to answer it.",
)

async def main():
    result = Runner.run_streamed(weather_agent, "What's the weather in UK?")
    async for chunk in result.stream_events():
        print(chunk, end="", flush=True)

asyncio.run(main())
```

### Overwrite trace attributes

If you are using `trace` context to set the trace attributes, the trace attributes such as the input will be the **input of the first agent run** while the output will be the **output of the last agent run**.

If you want to set override the input, output, or any other attribute for the current trace, you can use the `update_current_trace` function:

```python main.py {10}
from deepeval.openai_agents import DeepEvalTracingProcessor
from agents import add_trace_processor, Agent, Runner, trace
from deepeval.tracing.context import update_current_trace

add_trace_processor(DeepEvalTracingProcessor())
agent = Agent(name="Assistant", instructions="You are a helpful assistant")

with trace(workflow_name="test_workflow_1", metadata={"test_metadata_1": "test_metadata_1"}):
    response_1 = Runner.run_sync(agent, "Hola, ¿cómo estás?")
    update_current_trace(
        name="New name",
        input="New input",
        output="New output",
        metadata={"New key": "New value"}
    )
```

<Note>
  The `thread_id` and `metadata` will ALSO override the `trace` context's
  `group_id` and `metadata` respectively.
</Note>

<Accordion title='View Trace Attributes'>
<ParamField path="name" type="str" required={false}>
  The name of the trace. [Learn more](/docs/llm-tracing/advanced-features/name).
</ParamField>

<ParamField path="tags" type="List[str]" required={false}>
  Tags are string labels that help you group related traces. [Learn
  more](/docs/llm-tracing/advanced-features/tags).
</ParamField>

<ParamField path="metadata" type="Dict" required={false}>
  Attach any metadata to the trace. [Learn
  more](/docs/llm-tracing/advanced-features/metadata).
</ParamField>

<ParamField path="thread_id" type="str" required={false}>
  Supply the thread or conversation ID to view and evaluate conversations.
  [Learn more](/docs/llm-tracing/advanced-features/threads).
</ParamField>

<ParamField path="user_id" type="str" required={false}>
  Supply the user ID to enable user analytics. [Learn
  more](/docs/llm-tracing/advanced-features/users).
</ParamField>

<Info>
  Each attribute is **optional**, and works the same way as the [native tracing
  features](/docs/llm-tracing/introduction) on Confident AI.
</Info>
</Accordion>

### Logging prompts

If you are managing prompts[/docs/llm-evaluation/prompt-management/version-prompts] on Confident AI and wish to log them, pass your `Prompt` object to the DeepEval's `Agent` wrapper.

```python main.py {13}
from agents import Runner, add_trace_processor
from deepeval.prompt import Prompt
from deepeval.openai_agents import DeepEvalTracingProcessor, Agent

add_trace_processor(DeepEvalTracingProcessor())

prompt = Prompt(alias="<prompt-alias>")
prompt.pull(version="00.00.01")

spanish_agent = Agent(
    name="Spanish agent",
    instructions=prompt.interpolate(),
    confident_prompt=prompt,
)
Runner.run_sync(spanish_agent, "¿Cómo estás?")
```

<Note>
  Logging prompts lets you attribute specific prompts to OpenAI Agent LLM spans.
  Be sure to **pull the prompt** before logging it, otherwise the prompt will
  not be visible on Confident AI.
</Note>

## Evals Usage

### Online evals

You can run [online evals](/docs/llm-evaluation/online-evals) on your OpenAI Agent, which will run evaluations on all incoming traces on Confident AI’s servers. This is the recommended approach, especially if your agent is in production.

<Steps>

<Step title="Create metric collection">

Create a metric collection on [Confident AI](https://app.confident.ai) with the metrics you wish to use to evaluate your OpenAI Agent.

<Accordion title='Click to see supported metrics for OpenAI Agents'>

Confident AI supports evaluating the input-output pairs of OpenAI Agent spans and traces, which means your metric collections must only contain metrics that only require the input and output for evaluation. These metrics include:

- [Answer Relevancy](https://www.confident-ai.com/docs/metrics/single-turn/answer-relevancy-metric)
- [Task Completion](https://www.confident-ai.com/docs/metrics/single-turn/task-completion-metric)
- [GEval](https://deepeval.com/docs/metrics-llm-evals)
- [Bias](https://www.confident-ai.com/docs/metrics/single-turn/bias-metric)
- [Toxicity](https://www.confident-ai.com/docs/metrics/single-turn/toxicity-metric)

<Info>
  If you're looking to use other metrics, setup Confident AI's [native
  tracing](/docs/llm-tracing/introduction) instead.
</Info>

</Accordion>
<Frame caption="Create metric collection">
  <video
    data-video="integrations.createMetricCollection"
    controls
    autoPlay
  />
</Frame>

</Step>

<Step title="Run evals">

You can run evals at both the trace and span level. We recommend creating separate [metric collections](/docs/metrics/metric-collections) for each component, since each requires its own evaluation criteria and metrics.

<Warning>
  Agent span metrics are currently only supported for `Runner.run` and
  `Runner.run_sync`.
</Warning>

<Tabs>

<Tab title="Agent Span">

Replace your `Agent` with DeepEval's and supply the metric collection name to run evals on the agent span level.

```python main.py {10}
import asyncio
from agents import Runner, add_trace_processor
from deepeval.openai_agents import Agent, DeepEvalTracingProcessor

add_trace_processor(DeepEvalTracingProcessor())

weather_agent = Agent(
    name="Weather Agent",
    instructions="You are a weather agent. You are given a question about the weather and you need to answer it.",
    agent_metric_collection="test_collection_1",
)

async def main():
    result = await Runner.run(weather_agent, "What's the weather in UK?")
    print(result.final_output)

asyncio.run(main())
```

</Tab>

<Tab title="LLM Span">

Replace your Pydantic `Agent` with DeepEval’s and your supply the metric collection name to `Agent` to run evals on the LLM span level.

```python main.py {10}
import asyncio
from agents import Runner, add_trace_processor
from deepeval.openai_agents import Agent, DeepEvalTracingProcessor

add_trace_processor(DeepEvalTracingProcessor())

weather_agent = Agent(
    name="Weather Agent",
    instructions="You are a weather agent. You are given a question about the weather and you need to answer it.",
    llm_metric_collection="test_collection_1",
)

async def main():
    result = await Runner.run(weather_agent, "What's the weather in UK?")
    print(result.final_output)

asyncio.run(main())
```

</Tab>

<Tab title="Tool Span">

Replace the `function_tool` decorator with DeepEval's wrapper, and provide the metric collection name to run evals on the tool span level.

```python main.py {7}
import asyncio
from agents import Agent, Runner, add_trace_processor
from deepeval.openai_agents import function_tool, DeepEvalTracingProcessor

add_trace_processor(DeepEvalTracingProcessor())

@function_tool(metric_collection="test_collection_1")
def get_weather(city: str) -> str:
    return "The weather in " + city + " is sunny."

weather_agent = Agent(
    name="Weather Agent",
    instructions="You are a weather agent. You are given a question about the weather and you need to answer it.",
    tools=[get_weather],
)

async def main():
    result = await Runner.run(weather_agent, "What's the weather in UK?")
    print(result.final_output)

asyncio.run(main())
```

</Tab>
</Tabs>

<Success>
  All incoming traces and spans will now be evaluated using metrics from your
  metric collection.
</Success>

</Step>
</Steps>
