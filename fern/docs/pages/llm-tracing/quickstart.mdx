---
title: LLM Tracing Quickstart
subtitle: Setup LLM observability for your application in less than 5 minutes
slug: llm-tracing/quickstart
---

## Overview

Confident AI allows anyone building with any framework, language, and LLMs to setup LLM observability through tracing. Every interaction traced is evals-native, powered by DeepEval. This means apart form tracking latency, cost, and error rates, you can also run evals on:

- Traces
- Spans, and
- Threads

The demo below demonstrates what it looks like on the platform:

<Tabs>
  <Tab title="Traces">
    Traces are a single execution of your LLM app, and running evals on traces
    is akin to the [end-to-end
    evals](/docs/llm-evaluation/single-turn/end-to-end) for single-turn
    evaluation in development.
    <Frame caption="LLM Tracing: Traces with Evals">
      <video
        data-video="tracing.traces"
        controls
      />
    </Frame>
  </Tab>
  <Tab title="Spans">
    Spans are individual components such as LLM calls, agents, retrieveres,
    tools, etc. that make up a trace. Running evals on spans is akin to the
    [component-level evals](/docs/llm-evaluation/single-turn/component-level)
    for multi-turn evaluation in development.
    <Frame caption="LLM Tracing: Spans with Evals">
      <video
        data-video="tracing.spans"
        controls
      />
    </Frame>
  </Tab>
  <Tab title="Threads">
    Threads are **a group of traces**, grouped together via a `thread_id` that
    you provide during tracing. It represents a multi-turn LLM interaction,
    exactly the same as [E2E multi-turn
    evals.](/docs/llm-evaluation/multi-turn/end-to-end)
    <Frame caption="LLM Tracing: Threads with Evals">
      <video
        data-video="tracing.threads"
        controls
      />
    </Frame>
  </Tab>
</Tabs>

## How It Works

You would setup LLM tracing either through one of our [integrations](/docs/integrations), or by decorating your LLM app for Python and Typescript users. Either way, Confident AI will create an execution hierarchy of your LLM app, as well as log all components that were called for each LLM invocation:

- **Trace**: The overall process of tracking and visualizing the execution flow of your LLM application
- **Span**: Individual units of work within your application (e.g., LLM calls, tool executions, retrievals)

Each observed function **CREATES A SPAN**, and **MANY SPANS MAKE UP A TRACE**. When you have tracing setup, you can run evaluations on both the trace and span level.

## Trace Your First LLM Call

<Tip icon="leaf">
  You'll need to get your API key as shown in the [setup and
  installation](/docs/setup-and-installation) section before continuing.
</Tip>

<Tabs>
  <Tab title="Python">
    <Steps>

      <Step title="Installation">
        Install DeepEval and setup your tracing environment:

          ```bash
          pip install -U deepeval
          ```
      </Step>

      <Step title="Login to Confident AI">
        Get your [Confident AI Project API](https://app.confident-ai.com) key and login:

        <CodeBlocks>

          ```bash title="Set Env"
          export CONFIDENT_API_KEY=YOUR-API-KEY
          ```
          ```python title="In code"
          from deepeval.tracing import trace_manager

          trace_manager.configure(
              confident_api_key="YOUR-API-KEY"
          )
          ```
        </CodeBlocks>
      </Step>

      <Step title="Setup Tracing">
        The `@observe` decorator logs whatever it decorates within and is the primary way to instrument your LLM app for tracing.

          ```python title="main.py" {6,17}
          from openai import OpenAI
          from deepeval.tracing import observe

          client = OpenAI()

          @observe()
          def llm_app(query: str) -> str:
              return client.chat.completions.create(
                  model="gpt-4o",
                  messages=[
                      {"role": "user", "content": query}
                  ]
              ).choices[0].message.content
              return

          # Call app to send trace to Confident AI
          llm_app("Write me a poem.")
          ```
        âœ… You just created a trace with a span inside it. Go to the Observatory to see your traces there.

<Note>
If your `llm_app` has more than one function, simply decorate those functions with `@observe` too.

In a later section, you'll learn how to create [spans that are LLM specific](/docs/llm-tracing/tracing-features/span-types#llm-span), which allow you to log things like token cost and model name automatically.

</Note>
      </Step>

    </Steps>

  </Tab>

  <Tab title="TypeScript">
    <Steps>

      <Step title="Installation">
        Install DeepEval and setup your tracing environment:
        <CodeBlocks>

          ```bash title="npm"
          npm install deepeval-ts
          ```
          ```bash title="yarn"
          yarn add deepeval-ts
          ```

        </CodeBlocks>
      </Step>

      <Step title="Login to Confident AI">
        Get your [Confident AI Project API](https://app.confident-ai.com) key and login.

        <CodeBlocks>
          ```js title="In code"
          import { traceManager } from 'deepeval-ts/tracing';

          traceManager.configure({
              confidentApiKey: "YOUR-API-KEY"
          })
          ```
          ```bash title="Set Env"
          export CONFIDENT_API_KEY=YOUR-API-KEY
          ```
        </CodeBlocks>
      </Step>

      <Step title="Setup Tracing">
        The `observe` wrapper is the primary way to instrument your LLM application for tracing.

          ```ts title="index.ts" focus={2,13,16}
          import OpenAI from 'openai';
          import { observe } from 'deepeval-ts/tracing';

          const llmApp = async (query: string) => {
            const openai = new OpenAI();
            const res = await openai.chat.completions.create({
              model: "gpt-4o",
              messages: [{ role: "user", content: query }],
            });
            return res.choices[0].message.content;
          };

          const observedLlmApp = observe({ fn: llmApp });

          // Call app to send trace to Confident AI
          observedLlmApp("Write me a poem.");
          ```

        âœ… You just created a trace with a span inside it. Go to the Observatory to see your traces there.

<Note>
  If your `llmApp` has more than one function, simply wrap those functions in
  `observe` too and call the wrapped functions in `llmApp` instead.

        In a later section, you'll learn how to create [spans that are LLM specific](/docs/llm-tracing/tracing-features/span-types#llm-span), which allow you to log things like token cost and model name automatically.

</Note>
      </Step>

    </Steps>

  </Tab>
</Tabs>

**Congratulations!** ðŸŽ‰ Now whenever you run your LLM app, all traces will be logged AND evaluated on Confident AI. Go to the the **Observatory** section on Confident AI to check it out.

<Tip>
If you don't see the trace, it is 99.99% because your program exited before the traces had a chance to get posted. Try setting `CONFIDENT_TRACE_FLUSH=YES` if this is the case:

```bash
CONFIDENT_TRACE_FLUSH=YES
```

</Tip>

In the next section, we will learn how to run evals for LLM tracing.
