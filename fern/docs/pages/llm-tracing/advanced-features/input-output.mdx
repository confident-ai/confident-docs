---
subtitle: Learn how to supply input and output of your LLM application in a trace
slug: llm-tracing/advanced-features/input-output
---

## Overview

Both traces and spans have inputs and outputs, which you can set dynamically within your application using the `update_current_span` and `update_current_trace` function respectively.

## Set Trace I/O At Runtime

You can set the `input` and `output` of a trace using the `update_current_trace` function:

<Tabs>
    <Tab title="Python" language="python">
            ```python title="main.py" {13}
            from openai import OpenAI
            from deepeval.tracing import observe, update_current_trace
            
            client = OpenAI()
            
            @observe()
            def llm_app(query: str):
                res = client.chat.completions.create(
                    model="gpt-4o",
                    messages=[{"role": "user", "content": query}]
                ).choices[0].message.content
            
                update_current_trace(input=query, output=res)
                return res
            
            llm_app("Write me a poem.")
            ```
    </Tab>
    <Tab title="TypeScript" language="typescript">
            ```typescript title="index.ts" {11}
            import OpenAI from 'openai';
            import { observe, updateCurrentTrace } from '@deepeval-ts/tracing';
            
            const openai = new OpenAI();
            
            const llmApp = async (query: string) => {
                const res = await openai.chat.completions.create({
                    model: "gpt-4o",
                    messages: [{ role: "user", content: query }],
                });
                updateCurrentTrace({ input: query, output: res.choices[0].message.content });
                return res.choices[0].message.content;
            };

            const observedLlmApp = observe({ fn: llmApp });
            observedLlmApp("Write me a poem.");
            ```
    </Tab>

</Tabs>

The `input` and `output` can be **ANY TYPE**, and is useful for visualization on the UI (even more so if you're using [conversation threads](/docs/llm-tracing/advanced-features/threads)).

### Big disclaimer for threads

**HOWEVER**, if you are planning to [create a thread](/docs/llm-tracing/advanced-features/threads) from the traces (for chatbots, conversations, etc. multi-turn use cases), it is highly recommended that you provide **strings** instead, where the `input` will represent the user input, and `output` representing the LLM generated output. You can also leave out any `input` or `output` for consecutive user/LLM behaviors.

You will also need the `input` and `output` to run [online evaluations on a thread](/docs/llm-tracing/evaluations#online-evals-for-threads), as these will be used as the turns for a conversational test case.

## Set Span I/O At Runtime

You can set the `input` and `output` on spans using the `update_current_span` function.

One thing to note however, is we recommend [setting attributes](/docs/llm-tracing/advanced-features/attributes) instead of `input` and `output` directly on spans that are one of the **DEFAULT span types**, since DeepEval handles the `input` and `output` setting automatically for default span `type`s.

<Note>
For example, the `"retriever"` span `type` expects a string as the `input` and list of strings as the `output`, which you might violate if setting I/O yourself. This will decrease the chances that you run into an error.
</Note>

<Tabs>
    <Tab title="Python" language="python">
            ```python title="main.py" {13}
            from openai import OpenAI
            from deepeval.tracing import observe, update_current_span
            
            client = OpenAI()
            
            @observe()
            def llm_app(query: str):
                res = client.chat.completions.create(
                    model="gpt-4o",
                    messages=[{"role": "user", "content": query}]
                ).choices[0].message.content
            
                update_current_span(input=query, output=res)
                return res
            
            llm_app("Write me a poem.")
            ```
    </Tab>
    <Tab title="TypeScript" language="typescript">
            ```typescript title="index.ts" {11}
            import OpenAI from 'openai';
            import { observe, updateCurrentSpan } from '@deepeval-ts/tracing';

            const openai = new OpenAI();

            const llmApp = async (query: string) => {
                const res = await openai.chat.completions.create({
                    model: "gpt-4o",
                    messages: [{ role: "user", content: query }],
                });
                updateCurrentSpan({ input: query, output: res.choices[0].message.content });
                return res.choices[0].message.content;
            };

            const observedLlmApp = observe({ fn: llmApp });
            observedLlmApp("Write me a poem.");
            ```
    </Tab>

</Tabs>

This example is the same as the one for tracing except for the `update_current_trace`, and that's not a mistake. You can set `input` and `output`s the same way as you do for traces, and if a trace's I/O is not set it defaults to the I/O of the root span.

The `input` and `output` can be **ANY TYPE** for custom span `type`s, and is useful for visualization on the UI.
