---
subtitle: Sending only part of your traces to Confident AI
slug: llm-tracing/advanced-features/sampling
---

## Overview

Sampling allows you to control what percentage of traces are sent to Confident's observatory.

<Info>
This is useful for high-volume applications where you may want to reduce the amount of data being sent while still maintaining visibility into your system's performance. 
</Info>

## Configure Sample Rate

Configure the sampling rate by setting the `CONFIDENT_SAMPLE_RATE` environment variable, which represents the proportion of traces that will be sent to the observatory.

```bash 
export CONFIDENT_SAMPLE_RATE=0.5 
```

Alternatively, you can set the sampling rate directly in code:

<Tabs>
    <Tab title="Python" language="python">
            ```python title="main.py" {5}
            from deepeval.tracing import observe, trace_manager
            from openai import OpenAI
            
            client = OpenAI()
            trace_manager.configure(sampling_rate=0.5)
            
            @observe()
            def llm_app(query: str):
                return client.chat.completions.create(
                    model="gpt-4o",
                    messages=[{"role": "user", "content": query}]
                ).choices[0].message.content
            
            for _ in range(10):
                llm_app("Write me a poem.")  # roughly half of these traces will be sent
            ```
    </Tab>
    <Tab title="TypeScript" language="typescript">
            ```typescript title="index.ts" {5}
            import { observe, traceManager } from 'deepeval-ts/tracing';
            import OpenAI from 'openai';

            const openai = new OpenAI();
            traceManager.configure({ sampleRate: 0.5 });

            const llmApp = async (query: string) => {
                const response = await openai.chat.completions.create({
                    model: "gpt-4o",
                    messages: [{ role: "user", content: query }]
                });
                return response.choices[0].message.content;
            };

            const observedLlmApp = observe({fn: llmApp});

            for (let i = 0; i < 10; i++) {
                observedLlmApp("Write me a poem."); // Roughly half of these traces will be sent
            }
            ```
    </Tab>

</Tabs>

<Note>
Traces are sampled at random and the rest are dropped
</Note>
