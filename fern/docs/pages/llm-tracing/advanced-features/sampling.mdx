---
subtitle: LLM Tracing Sampling
slug: llm-tracing/advanced-features/sampling
description: Learn about sampling in LLM Tracing
---

## Overview

Sampling allows you to control what percentage of traces are sent to Confident's observatory.

<Info>
This is useful for high-volume applications where you may want to reduce the amount of data being sent while still maintaining visibility into your system's performance. 
</Info>

## Configure Sample Rate

Configure the sampling rate by setting the `CONFIDENT_SAMPLE_RATE` environment variable, which represents the proportion of traces that will be sent to the observatory.

<CodeBlock>```bash export CONFIDENT_SAMPLE_RATE=0.5 ```</CodeBlock>

Alternatively, you can set the sampling rate directly in code:

<Tabs>
    <Tab title="Python">
        <CodeBlock>
            ```python title="main.py" {5}
            from deepeval.tracing import observe, trace_manager
            from openai import OpenAI
            
            client = OpenAI()
            trace_manager.configure(sampling_rate=0.5)
            
            @observe()
            def llm_app(query: str):
                return client.chat.completions.create(
                    model="gpt-4o",
                    messages=[{"role": "user", "content": query}]
                ).choices[0].message.content
            
            for _ in range(10):
                llm_app("Write me a poem.")  # roughly half of these traces will be sent
            ```
        </CodeBlock>
    </Tab>
    <Tab title="Js/TypeScript">
        <CodeBlock>
            ```typescript title="index.ts" {5}
            import { observe, traceManager } from '@deepeval-ts/tracing';
            import OpenAI from 'openai';

            const openai = new OpenAI();
            traceManager.configure({ sampleRate: 0.5 });

            const llmApp = (query: string) => {
            const response = openai.chat.completions.create({
                model: "gpt-4o",
                messages: [{ role: "user", content: query }]
            });
            return response.choices[0].message.content;
            };

            const observedLlmApp = observe({fn: llmApp});

            for (let i = 0; i < 10; i++) {
            observedLlmApp("Write me a poem."); // Roughly half of these traces will be sent
            }
            ```
        </CodeBlock>
    </Tab>

</Tabs>

<Note>
Traces are sampled at random and the rest are dropped
</Note>
