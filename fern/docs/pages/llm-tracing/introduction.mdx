---
title: Introduction to LLM Tracing
subtitle: Instrument your LLM applications for observability and evaluate them on a component level.
slug: llm-tracing/introduction
description: Learn about LLM tracing with Confident AI
---

## Overview

Confident AI offers **LLM tracing** for teams to instrument, observe, and quality-monitor AI apps. Think Datadog for AI apps, but with an additional suite of 50+ evaluation metrics to monitor AI quality over time.

There are three ways to instrument your app for tracing on Confident AI:

| Approach                                                     | Best For                                                                                                                                                                                                     | Language Support                           |
| ------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------ |
| **`observe` decorator/wrapper**                              | Full control over spans, attributes, and trace structure                                                                                                                                                     | Python, TypeScript                         |
| **Third-party integrations**                                 | Auto-instrument popular frameworks ([OpenAI](/docs/integrations/third-party/openai), [LangChain](/docs/integrations/third-party/langchain), [Pydantic AI](/docs/integrations/third-party/pydantic-ai), etc.) | Python and/or Typescript                   |
| [**OpenTelemetry (OTEL)**](/docs/integrations/opentelemetry) | Language-agnostic, standards-based instrumentation                                                                                                                                                           | Python, TypeScript, Go, Ruby, C#, and more |

## Why LLM Tracing?

Tracing is the underlying infrastructure that powers online evals, which allows you to **monitor AI quality in production**. and there are three core data types to be aware of:

- **Traces** — a single end-to-end execution of your LLM app
- **Spans** — individual components within a trace (e.g., LLM calls, retrievals, tool executions)
- **Threads** — a group of traces representing a multi-turn conversation

By instrumenting your application, every execution is captured and all three can be automatically evaluated against your metrics.

<Tabs>
  <Tab title="Traces">

    <Frame caption="LLM Tracing: Traces with Evals">
      <video data-video="tracing.traces" controls />
    </Frame>

  </Tab>
  <Tab title="Spans">

    <Frame caption="LLM Tracing: Spans with Evals">
      <video data-video="tracing.spans" controls />
    </Frame>

  </Tab>
  <Tab title="Threads">

    <Frame caption="LLM Tracing: Threads with Evals">
      <video data-video="tracing.threads" controls />
    </Frame>

  </Tab>
</Tabs>

## Get started

Instrument your LLM application using the `@observe` decorator, one-line integrations, or OpenTelemetry (OTEL):

<CardGroup cols={2}>
  <Card
    title="5 Min Quickstart"
    icon="fa-light fa-bolt"
    iconType="solid"
    href="/llm-tracing/quickstart"
  >
    <div className='card-link-icon'>
      <Icon icon="arrow-up-right-from-square" />
    </div>

    Manually instrument your LLM app start tracing in minutes, in either Python or Typescript.

  </Card>
  <Card
    title="Integrations & OTEL"
    icon="fa-light fa-plug"
    iconType="solid"
    href="/integrations"
  >
    <div className='card-link-icon'>
      <Icon icon="arrow-up-right-from-square" />
    </div>

    Auto-instrument with one-line integrations for OpenAI, LangChain, and more — or use OpenTelemetry for any language.

  </Card>
</CardGroup>

## Key capabilities

<CardGroup cols={2}>
  <Card title="Online Evals" icon="toggle-on">
    Run evaluations on traces, spans, and threads in real-time as they're
    ingested, or retrospectively.
  </Card>
  <Card title="Latency & Cost Tracking" icon="stopwatch">
    Automatically track execution time and token costs across every span in your
    application.
  </Card>
  <Card title="Customize Traces" icon="pen-to-square">
    Enrich traces with tags, metadata, user IDs, and thread groupings for
    filtering and attribution.
  </Card>
  <Card title="Trace Management" icon="sliders">
    Route traces to projects, separate by environment, mask PII, and control
    sampling rates.
  </Card>
</CardGroup>

## Learn the fundamentals

New to LLM tracing? These concepts will help you get the most out of your setup:

- [Span Types](/docs/llm-tracing/features/span-types) — classify spans as LLM, retriever, tool, or agent
- [Input/Output](/docs/llm-tracing/features/input-output) — control what data is captured on traces and spans
- [Threads](/docs/llm-tracing/features/threads) — group traces into multi-turn conversations

<AccordionGroup>
<Accordion title="How will tracing affect my app?">

Confident AI tracing is designed to be completely non-intrusive to your application. It:

- Can be disabled/enabled anytime through the `CONFIDENT_TRACING_ENABLED="YES"/"NO"` environment variable.
- Requires no rewrite of your existing code - just add the `@observe` decorator.
- Runs asynchronously in the background with zero impact on latency.
- Fails silently if there are any issues, ensuring your app keeps running.
- Works with any function signature - you can set input/output at runtime.

</Accordion>
<Accordion title="What languages and frameworks are supported?">

For the `@observe` decorator, **Python** and **TypeScript** are supported. For integrations, Python is supported with TypeScript coming soon. Via **OpenTelemetry**, you can instrument in any language — including Python, TypeScript, Go, Ruby, and C#. See the [Integrations & OTEL](/docs/integrations) page for the full list.

</Accordion>
</AccordionGroup>
