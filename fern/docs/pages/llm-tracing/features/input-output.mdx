---
subtitle: Learn how to supply input and output of your LLM application in a trace
slug: llm-tracing/features/input-output
---

## Overview

Both traces and spans have inputs and outputs, which you can set dynamically within your application using the `update_current_span`/`updateCurrentSpan` and `update_current_trace`/`updateCurrentTrace` function respectively.

<Tip>

Setting the I/O on traces is also important for threads view on Confident AI.

</Tip>

## Set Trace I/O

By default, the input and output of a trace is defaulted to the input arguments of the first span and the output of the last span you've wrapped/decorated. You can however override the `input` and `output` on spans at runtime.

<Tabs>
    <Tab title="Python" language="python">
            ```python title="main.py" {13}
            from openai import OpenAI
            from deepeval.tracing import observe, update_current_trace
            
            client = OpenAI()
            
            @observe()
            def llm_app(query: str):
                res = client.chat.completions.create(
                    model="gpt-4o",
                    messages=[{"role": "user", "content": query}]
                ).choices[0].message.content
            
                update_current_trace(input=query, output=res)
                return res
            
            llm_app("Write me a poem.")
            ```
    </Tab>
    <Tab title="TypeScript" language="typescript">
            ```typescript title="index.ts" {11}
            import OpenAI from 'openai';
            import { observe, updateCurrentTrace } from 'deepeval-ts/tracing';
            
            const openai = new OpenAI();
            
            const llmApp = async (query: string) => {
                const res = await openai.chat.completions.create({
                    model: "gpt-4o",
                    messages: [{ role: "user", content: query }],
                });
                updateCurrentTrace({ input: query, output: res.choices[0].message.content });
                return res.choices[0].message.content;
            };

            const observedLlmApp = observe({ fn: llmApp });
            observedLlmApp("Write me a poem.");
            ```
    </Tab>

</Tabs>

The `input` and `output` can be **ANY TYPE**, and is useful for visualization on the UI (even more so if you're using [conversation threads](/docs/llm-tracing/features/threads)).

## Set Span I/O

By default, the input and output of a span is defaulted to the input arguments and output of the function/method you're mapping. You can however override the `input` and `output` on spans at runtime.

<Note>
  For example, the `"retriever"` span `type` expects a string as the `input` and
  list of strings as the `output`, which you might violate if setting I/O
  yourself. This will decrease the chances that you run into an error.
</Note>

<Tabs>
    <Tab title="Python" language="python">
            ```python title="main.py" {13}
            from openai import OpenAI
            from deepeval.tracing import observe, update_current_span
            
            client = OpenAI()
            
            @observe()
            def llm_app(query: str):
                res = client.chat.completions.create(
                    model="gpt-4o",
                    messages=[{"role": "user", "content": query}]
                ).choices[0].message.content
            
                update_current_span(input=query, output=res)
                return res
            
            llm_app("Write me a poem.")
            ```
    </Tab>
    <Tab title="TypeScript" language="typescript">
            ```typescript title="index.ts" {11}
            import OpenAI from 'openai';
            import { observe, updateCurrentSpan } from 'deepeval-ts/tracing';

            const openai = new OpenAI();

            const llmApp = async (query: string) => {
                const res = await openai.chat.completions.create({
                    model: "gpt-4o",
                    messages: [{ role: "user", content: query }],
                });
                updateCurrentSpan({ input: query, output: res.choices[0].message.content });
                return res.choices[0].message.content;
            };

            const observedLlmApp = observe({ fn: llmApp });
            observedLlmApp("Write me a poem.");
            ```
    </Tab>

</Tabs>

This example is the same as the one for tracing except for the `update_current_trace`, and that's not a mistake. You can set `input` and `output`s the same way as you do for traces, and if a trace's I/O is not set it defaults to the I/O of the root span.

The `input` and `output` can be **ANY TYPE** for custom span `type`s, and is useful for visualization on the UI.

## I/O for Threads

For multu-turn AI apps that [create a thread](/docs/llm-tracing/features/threads) from the traces, it is highly recommended that you provide the **strings** instead, where the `input` will represent the user input, and `output` representing the AI generated output. You can also leave out any `input` or `output` for consecutive user/LLM behaviors.

You will also need the `input` and `output` to run [online evaluations on a thread](/docs/llm-tracing/evaluate-threads), as these will be used as the turns for a conversational test case.

## Next Steps

With your trace and span I/O configured, connect traces into conversations or start evaluating them.

<CardGroup cols={2}>
  <Card
    title="Thread Traces"
    href="/llm-tracing/features/threads"
    icon="messages"
  >
    Group traces into threads to track multi-turn conversations and evaluate
    entire workflows.
  </Card>
  <Card
    title="Online Evaluations"
    href="/llm-tracing/online-evals"
    icon="toggle-on"
  >
    Run evaluations on traces, spans, and threads in real-time as they're
    ingested into Confident AI.
  </Card>
</CardGroup>
