---
subtitle: Pull datasets to use them for evaluation
slug: llm-evaluation/dataset-management/using-datasets
---

## Overview

In the previous section, we learnt how to [curate datasets](/llm-evaluation/dataset-management/create-goldens) by manging goldens through the platform directly or via Confident's Evals API. In this section, we will learn how to:

- Pull dataests for LLM testing
- Associate dataset with test runs
- Use the `evals_iterator` to run evals on datasets (for python users)

## Pull Goldens via Evals API

Datasets are either single or multi-turn, and you should know that pulling a single-turn dataset will give you single-turn goldens, and vice versa.

<Note>
  You will be responsible for mapping single-turn goldens to single-turn test
  cases, and vice versa.
</Note>

Pulling goldens via the Evals API will only pull **finalized** goldens by default.

<Tabs>

<Tab title="Python" language="python">

<Steps>

<Step title="Pull goldens">

First use the `.pull()` method:

```python main.py
from deepeval.dataset import EvaluationDataset

dataset = EvaluationDataset()
dataset.pull(alias="YOUR-DATASET-ALIAS")

print(dataset.goldens) # Check it's pulled correctly
```

</Step>

<Step title="Construct test cases">

Then loop through your dataset of goldens to create a list of test cases:

```python main.py focus={7-13}
from deepeval.dataset import EvaluationDataset
from deepeval.test_case import LLMTestCase

dataset = EvaluationDataset()
dataset.pull(alias="YOUR-DATASET-ALIAS")

for golden in dataset.goldens:
    test_case = LLMTestCase(
        input=golden.input,
        actual_output=llm_app(golden.input),
        # map any additional fields here
    )
    dataset.add_test_case(test_case)
```

<Tip>
  For **multi-turn** datasets, you will create `ConversationalTestCase`s instead:

```python main.py
from deepeval.test_case import ConversationalTestCase

for golden in dataset.goldens:
  test_case = simulate(golden) # simulate conversation
  dataset.add_test_case(test_case)
```

</Tip>

</Step>

<Step title="Run an evaluation">

By calling `.add_test_case()` in the previous step, each time you run evaluate Confident AI will automatically associate any created test run with your dataset:

```python
from deepeval import evaluate

evaluate(test_cases=dataset.test_cases, metrics=[...])
```

</Step>

</Steps>

</Tab>
<Tab title="Typescript" language="typescript">

<Steps>

<Step title="Pull goldens">

First use the `.pull()` method:

```ts index.ts
import { EvaluationDataset } from "deepeval-ts";

dataset = new EvaluationDataset();
dataset.pull({ alias: "YOUR-DATASET-ALIAS" });

console.log(dataset.goldens);
```

</Step>

<Step title="Construct test cases">

Then loop through your dataset of goldens to create a list of test cases:

```ts index.ts focus={7-13}
import { EvaluationDataset } from "deepeval-ts";
import { LLMTestCase } from "deepeval-ts";

dataset = new EvaluationDataset();
dataset.pull({ alias: "YOUR-DATASET-ALIAS" });

for (const golden of dataset.goldens) {
  const testCase = new LLMTestCase({
    input: golden.input,
    actualOutput: llmApp(golden.input),
    // map any additional fields here
  });
  dataset.addTestCase(testCase);
}
```

<Tip>
  For **multi-turn** datasets, you will create `ConversationalTestCase`s instead:

```ts index.ts
import { ConversationalTestCase } from "deepeval-ts";

for (const golden of dataset.goldens) {
  const testCase = simulate(golden); // simulate conversation
  dataset.addTestCase(testCase);
}
```

</Tip>

</Step>

<Step title="Run an evaluation">

By calling `.addTestCase()` in the previous step, each time you run evaluate Confident AI will automatically associate any created test run with your dataset:

```ts
import { evaluate } from "deepeval-ts";

evaluate({ testCases: dataset.testCases, metrics: [...]});
```

</Step>

</Steps>

</Tab>
<Tab title="curL" language="curl">

<Steps>

<Step title="Pull goldens">

First, pull goldens using the `/v1/datasets` endpoint.

<EndpointRequestSnippet
  endpoint="GET /v1/datasets"
/>

</Step>

<Step title="Construct test cases">

Construct a JSON array of test cases from the goldens you pulled, preserving the golden fields.

<Tabs>
  <Tab title="Single-Turn">
    ```json
    [
      {
        "input": "How tall is Mount Everest?",
        // Replace with your LLM app output
        "actualOutput": "Mount Everest is 9K meters tall."
      }
    ]
    ```
    <Accordion title="Click here to see the parameters for creating a single-turn test case">
      <Card title="Parameters of `LLMTestCase`" href="/docs/api-reference/evaluation/evaluate-llm#request.body.llmTestCases">
      <br />
        - input: `string`
        - actualOutput: `string`
        - name: `string`
        - expectedOutput: `string`
        - retrievalContext: `list of strings`
        - context: `list of strings`
        - toolsCalled: `list of ToolCall`
        - expectedTools: `list of ToolCall`
      </Card>
    </Accordion>
  </Tab>
  <Tab title="Multi-Turn">
    ```json
    [
      {
        "scenario": "User asking about Mount Everest height.",
        "turns": [
          { "role": "user", "content": "How tall is Mount Everest?" },
          { "role": "assistant", "content": "Mount Everest is 9K meters tall." }
          // Replace with your LLM app outputs
        ],
      }
    ]
    ```
    <Accordion title="Click here to see the parameters for creating a multi-turn test case">
      <Card title="Parameters of `ConversationalTestCase`" href="/docs/api-reference/evaluation/evaluate-llm#request.body.conversationalTestCases">
      <br />
        - turns: `list of Turn`
        - scenario: `string`
        - name: `string`
        - expectedOutput: `string`
        - userDescription: `string`
        - chatbotRole: `string`
      </Card>
    </Accordion>
  </Tab>
</Tabs>

</Step>

<Step title="Create metric collection">

Create a metric collection through `v1/metric-collections`.

<EndpointRequestSnippet
  endpoint="POST /v1/metric-collections"
/>

</Step>

<Step title="Run an evaluation">

Run an evaluation using the test cases you constructed and metric collection you created using `/v1/evaluate`.

<EndpointRequestSnippet
  endpoint="POST /v1/evaluate"
/>

</Step>

</Steps>

</Tab>

</Tabs>

## Using Evals Iterator

Typeically, you would just provide your dataset as a list of test cases for evaluatioin. However, if you're running **single-turn, end-to-end OR component-level** evaluations, and is using `deepeval` in Python, you can use the `evals_iterator()` instead:

```python main.py
from deepeval.dataset import EvaluationDataset

dataset = EvaluationDataset()
dataset.pull(alias="YOUR-DATASET-ALIAS")

for golden in dataset.evals_iterator():
    llm_app(golden.input) # Replace with your LLM app

# Async version
# import asyncio
#
# for golden in dataset.evals_iterator():
#    task = asyncio.create_task(a_llm_app(golden.input))
#    dataset.evaluate(task)
```

You'll need to trace your LLM app to make this work. Read this section on running [single-turn end-to-end evals with tracing](/llm-evaluation/single-turn/end-to-end#llm-tracing-for-local-evals) to learn more.
