---
subtitle: Pull datasets locally to use them for evaluation.
slug: llm-evaluation/dataset-management/using-datasets
---

## Overview

In the previous section, we learnt how to push and queue goldens via Confident's Evals API. In this section, we will learn how to:

- Pull single and multi-turn datasets for evaluation
- Access custom column values from goldens
- Parse multi-modal goldens (images) into an evaluatable format
- Use the `evals_iterator` to run evals on single-turn datasets (Python only)

## How it works

Code-driven evals follow a similar process to [no-code evals](/llm-evaluation/no-code-evals/single-turn-evals#how-it-works), but you control the evaluation loop:

1. **Pull dataset** — fetch goldens from Confident AI using the Evals API
2. **Invoke AI app** — call your AI app with each golden's input
3. **Create test cases** — map golden fields and AI outputs into test cases
4. **Run evaluation** — execute metrics on your test cases and push results

Here's a visual representation of the data flow:

```mermaid
sequenceDiagram
    participant You as Your Code
    participant Platform as Confident AI
    participant AI as Your AI App
    participant Metrics as Local/Remote Metrics

    You->>Platform: Pull dataset (goldens)
    Platform-->>You: Return goldens

    loop For each golden in dataset
        You->>AI: Invoke with golden.input
        AI-->>You: Generate output
        You->>You: Create test case from golden + output
    end

    You->>Metrics: Run evaluation on test cases
    Metrics-->>You: Metric scores
    You->>Platform: Push test run results
    Platform-->>You: Test run created
```

The key difference from no-code evals is that **you control the evaluation
loop** — pulling goldens, invoking your AI app, and constructing test cases
all happen in your code.

<Tip>
You can manage your datasets in any project by configuring a `CONFIDENT_API_KEY`.

- For default usage, set `CONFIDENT_API_KEY` as an environment variable.
- To target a specific project, pass a `confident_api_key` directly when creating the `EvaluationDataset`.

```python
from deepeval.dataset import EvaluationDataset

dataset = EvaluationDataset(confident_api_key="confident_us...")
dataset.delete(alias="YOUR-DATASET-ALIAS")
```

When both are provided, the `confident_api_key` passed to `EvaluationDataset` always takes precedence over the environment variable.

</Tip>

## Pull Goldens via Evals API

Datasets are either single or multi-turn, and you should know that pulling a single-turn dataset will give you single-turn goldens, and vice versa.

<Note>
  You will be responsible for mapping single-turn goldens to single-turn test
  cases, and vice versa.
</Note>

Pulling goldens via the Evals API will only pull **finalized** goldens by default. Below is a single-turn dataset example ([click here](/docs/llm-evaluation/code-driven/multi-turn) for multi-turn usage of datasets):

<Tabs>

<Tab title="Python" language="python">

<Steps>

<Step title="Pull goldens">

First use the `.pull()` method:

```python main.py
from deepeval.dataset import EvaluationDataset

dataset = EvaluationDataset()
dataset.pull(alias="YOUR-DATASET-ALIAS")

print(dataset.goldens) # Check it's pulled correctly
```

</Step>

<Step title="Construct test cases">

Then loop through your dataset of goldens to create a list of test cases:

```python main.py focus={7-13}
from deepeval.dataset import EvaluationDataset
from deepeval.test_case import LLMTestCase

dataset = EvaluationDataset()
dataset.pull(alias="YOUR-DATASET-ALIAS")

for golden in dataset.goldens:
    test_case = LLMTestCase(
        input=golden.input,
        actual_output=llm_app(golden.input),
        # map any additional fields here
    )
    dataset.add_test_case(test_case)
```

<Tip>
  For **multi-turn** datasets, you will create `ConversationalTestCase`s instead:

```python main.py
from deepeval.test_case import ConversationalTestCase

for golden in dataset.goldens:
  test_case = simulate(golden) # simulate conversation
  dataset.add_test_case(test_case)
```

</Tip>

</Step>

<Step title="Run an evaluation">

By calling `.add_test_case()` in the previous step, each time you run evaluate Confident AI will automatically associate any created test run with your dataset:

```python
from deepeval import evaluate

evaluate(test_cases=dataset.test_cases, metrics=[...])
```

</Step>

</Steps>

</Tab>
<Tab title="Typescript" language="typescript">

<Steps>

<Step title="Pull goldens">

First use the `.pull()` method:

```ts index.ts
import { EvaluationDataset } from "deepeval-ts";

const dataset = new EvaluationDataset();
dataset.pull({ alias: "YOUR-DATASET-ALIAS" });

console.log(dataset.goldens);
```

</Step>

<Step title="Construct test cases">

Then loop through your dataset of goldens to create a list of test cases:

```ts index.ts focus={7-13}
import { EvaluationDataset, Golden, LLMTestCase } from "deepeval-ts";

const dataset = new EvaluationDataset();
dataset.pull({ alias: "YOUR-DATASET-ALIAS" });

for (const golden of dataset.goldens as Golden[]) {
  const testCase = new LLMTestCase({
    input: golden.input,
    actualOutput: llmApp(golden.input),
    // map any additional fields here
  });

  dataset.addTestCase(testCase);
}
```

<Tip>
  For **multi-turn** datasets, you will create `ConversationalTestCase`s instead:

```ts index.ts
import {
  ConversationalGolden,
  ConversationSimulator,
  EvaluationDataset,
} from "deepeval-ts";

const dataset = new EvaluationDataset();
dataset.pull({ alias: "YOUR-DATASET-ALIAS" });

const simulator = new ConversationSimulator({ modelCallback: chatbotCallback });
const testCases = await simulator.simulate({
  conversationalGoldens: dataset.goldens as ConversationalGolden[],
});

for (const testCase of testCases) {
  dataset.addTestCase(testCase);
}
```

</Tip>

</Step>

<Step title="Run an evaluation">

By calling `.addTestCase()` in the previous step, each time you run evaluate Confident AI will automatically associate any created test run with your dataset:

```ts
import { ConversationalTestCase, EvaluationDataset, evaluate } from "deepeval-ts";

const dataset = new EvaluationDataset();
dataset.pull({ alias: "YOUR-DATASET-ALIAS" });

evaluate({
  conversationalTestCases: dataset.testCases as ConversationalTestCase[],
  metrics: [...],
});
```

</Step>

</Steps>

</Tab>
<Tab title="curL" language="curl">

<Steps>

<Step title="Pull goldens">

First, pull goldens using the `/v1/datasets` endpoint.

<EndpointRequestSnippet endpoint="GET /v1/datasets/{alias}" />

</Step>

<Step title="Construct test cases">

Construct a JSON array of test cases from the goldens you pulled, preserving the golden fields.

<Tabs>
  <Tab title="Single-Turn">
    ```json
    [
      {
        "input": "How tall is Mount Everest?",
        // Replace with your LLM app output
        "actualOutput": "Mount Everest is 9K meters tall."
      }
    ]
    ```
    <Accordion title="Click here to see the parameters for creating a single-turn test case">
      <Card title="Parameters of `LLMTestCase`" href="/docs/api-reference/evaluation/evaluate-llm#request.body.llmTestCases">
      <br />
        - input: `string`
        - actualOutput: `string`
        - name: `string`
        - expectedOutput: `string`
        - retrievalContext: `list of strings`
        - context: `list of strings`
        - toolsCalled: `list of ToolCall`
        - expectedTools: `list of ToolCall`
      </Card>
    </Accordion>
  </Tab>
  <Tab title="Multi-Turn">
    ```json
    [
      {
        "scenario": "User asking about Mount Everest height.",
        "turns": [
          { "role": "user", "content": "How tall is Mount Everest?" },
          { "role": "assistant", "content": "Mount Everest is 9K meters tall." }
          // Replace with your LLM app outputs
        ],
      }
    ]
    ```
    <Accordion title="Click here to see the parameters for creating a multi-turn test case">
      <Card title="Parameters of `ConversationalTestCase`" href="/docs/api-reference/evaluation/evaluate-llm#request.body.conversationalTestCases">
      <br />
        - turns: `list of Turn`
        - scenario: `string`
        - name: `string`
        - expectedOutput: `string`
        - userDescription: `string`
        - chatbotRole: `string`
      </Card>
    </Accordion>
  </Tab>
</Tabs>

</Step>

<Step title="Create metric collection">

Create a metric collection through `v1/metric-collections`.

<EndpointRequestSnippet endpoint="POST /v1/metric-collections" />

</Step>

<Step title="Run an evaluation">

Run an evaluation using the test cases you constructed and metric collection you created using `/v1/evaluate`.

<EndpointRequestSnippet endpoint="POST /v1/evaluate" />

</Step>

</Steps>

</Tab>

</Tabs>

## Using Custom Columns

If your dataset has custom columns, you can access them via the `custom_column_key_values` field on each golden:

<Tabs>

<Tab title="Python" language="python">

```python main.py
from deepeval.dataset import EvaluationDataset
from deepeval.test_case import LLMTestCase

dataset = EvaluationDataset()
dataset.pull(alias="YOUR-DATASET-ALIAS")

for golden in dataset.goldens:
    # Access custom column values
    difficulty = golden.custom_column_key_values.get("difficulty")
    category = golden.custom_column_key_values.get("category")

    # Use them in your test case or LLM app invocation
    test_case = LLMTestCase(
        input=golden.input,
        actual_output=llm_app(golden.input, difficulty=difficulty),
    )
    dataset.add_test_case(test_case)
```

</Tab>

<Tab title="Typescript" language="typescript">

```ts index.ts
import { EvaluationDataset, Golden, LLMTestCase } from "deepeval-ts";

const dataset = new EvaluationDataset();
await dataset.pull({ alias: "YOUR-DATASET-ALIAS" });

for (const golden of dataset.goldens as Golden[]) {
  // Access custom column values
  const difficulty = golden.customColumnKeyValues?.difficulty;
  const category = golden.customColumnKeyValues?.category;

  // Use them in your test case or LLM app invocation
  const testCase = new LLMTestCase({
    input: golden.input,
    actualOutput: await llmApp(golden.input, { difficulty }),
  });
  dataset.addTestCase(testCase);
}
```

</Tab>

</Tabs>

## Using Images

Any (list of) golden text fields (such as input, scenario, etc.) that contains an image will be in the format of `[DEEPEVAL:IMAGE:url]`. The `url` inside the `[DEEPEVAL:IMAGE:url]` format is a public url that can be accessed by anyone.

For goldens containing images, here you can parse and use it accordingly as follows:

<Tabs>

<Tab title="Python" language="python">

The `deepeval` python SDK offers a utility method called `convert_to_multi_modal_array`. This method is useful for converting a string containing images in the `[DEEPEVAL:IMAGE:url]` format into a list of strings and `MLLMImage` items.

```python
from deepeval.dataset import EvaluationDataset
from deepeval.utils import convert_to_multi_modal_array

dataset = EvaluationDataset()
dataset.pull(alias="My Evals Dataset")

for golden in dataset.goldens:
    multimodal_array = convert_to_multi_modal_array(golden.input)
```

The `multimodal_array` here is a list containing strings and `MLLMImage`s, you can loop over this array to construct a messages array with images to pass to your MLLM. Here's an example showing how to construct messages array for `openai`:

```python maxLines=0
messages = []
for element in multimodal_array:
    if isinstance(element, str):
        messages.append({"type": "text", "text": element})
    elif isinstance(element, MLLMImage):
      if element.url:
          messages.append(
              {
                  "type": "image_url",
                  "image_url": {"url": element.url},
              }
          )
```

</Tab>

<Tab title="Typescript" language="typescript">

You can use a custom method to parse strings with images in `[DEEPEVAL:IMAGE:url]` format to convert them into an array of strings and URLs

```typescript maxLines=0
const parseMultimodalString = (s: string) => {
  const PATTERN = /\[DEEPEVAL:IMAGE:(.*?)\]/g;

  const result = [];
  let lastEnd = 0;
  let match;

  while ((match = PATTERN.exec(s)) !== null) {
    const start = match.index;
    const end = PATTERN.lastIndex;

    if (start > lastEnd) {
      result.push(s.slice(lastEnd, start));
    }

    const imageUrl = match[1];
    result.push({ url: imageUrl });

    lastEnd = end;
  }

  if (lastEnd < s.length) {
    result.push(s.slice(lastEnd));
  }

  return result;
}
```

You can now use this method to get `multimodalArray` and construct messages array to pass it to your MLLM. Here's an example on how to use it to construct `openai` format messages:

```typescript
const multimodalArray = parseMultimodalString(golden.input);

const messages = [];

for (const element of multimodalArray) {
  if (typeof element === "string") {
    messages.push({
      type: "text",
      text: element,
    });
  } else if (element.url) {
    messages.push({
      type: "image_url",
      image_url: { url: element.url },
    });
  }
}
```

</Tab>

<Tab title="curL" language="curl">

<EndpointRequestSnippet endpoint="GET /v1/datasets/{alias}" />

The dataset pulled here has images inside golden fields with the pattern `[DEEPEVAL:IMAGE:url]`. Please parse the fields to fetch the public `url` and use it as necessary.

</Tab>

<Tab title="Custom" language="custom">

You can use a custom method to parse strings with images in `[DEEPEVAL:IMAGE:url]` format to convert them into an array of strings and URLs

```python
def parse_multimodal_string(s: str):
    PATTERN = r"\[DEEPEVAL:IMAGE:(.*?)\]"
    matches = list(re.finditer(pattern, s))

    result = []
    last_end = 0
    for m in matches:
        start, end = m.span()
        if start > last_end:
          result.append(s[last_end:start])

        image_url = m.group(1)

        result.append({"url": image_url})
        last_end = end

    if last_end < len(s):
        result.append(s[last_end:])
        return result
```

You can use this method to get `multimodal_array` and construct messages array to pass it to your MLLM. Here's an example on how to use it:

```python
multimodal_array = parse_multimodal_string(golden.input)

messages = []
for element in multimodal_array:
    if isinstance(element, str):
        messages.append({"type": "text", "text": element})
    else:
        if element.get("url") is not None:
          messages.append(
              {
                  "type": "image_url",
                  "image_url": {"url": element.url},
              }
          )
```

</Tab>

</Tabs>

<Tip>

This is only required when using datasets in code - Confident AI automatically handles image parsing and conversion on the platform.

</Tip>

<Note>

`deepeval`'s native models like `GPTModel`, `GeminiModel` automatically parse the images inside the `[DEEPEVAL:IMAGE:url]` formats for you, you can simply pass any golden field with images inside the `.generate()` or `.a_generate()` methods and `deepeval` automatically handles images for you internally!

```python
from deepeval.models import GPTModel
from deepeval.dataset import EvaluationDataset

dataset = EvaluationDataset()
dataset.pull(alias="My Evals Dataset")

model = GPTModel(model="gpt-5.2")

for golden in dataset.goldens:
  print(model.generate(golden.input)) # Images are automatically handled by deepeval
```

</Note>


## Using Evals Iterator

Typically, you would just provide your dataset as a list of test cases for evaluation. However, if you're running **single-turn, end-to-end OR component-level** evaluations and using `deepeval` in Python, you can use the `evals_iterator()` instead:

```python main.py
from deepeval.dataset import EvaluationDataset

dataset = EvaluationDataset()
dataset.pull(alias="YOUR-DATASET-ALIAS")

for golden in dataset.evals_iterator():
    llm_app(golden.input) # Replace with your LLM app

# Async version
# import asyncio
#
# for golden in dataset.evals_iterator():
#    task = asyncio.create_task(a_llm_app(golden.input))
#    dataset.evaluate(task)
```

You'll need to trace your LLM app to make this work. Read this section on running [single-turn end-to-end evals with tracing](/docs/llm-evaluation/single-turn/end-to-end#llm-tracing-for-e2e-evals) to learn more.

## Datasets in CI/CD

Using datasets in CI/CD follows the same pattern as local evaluation — pull your dataset, create test cases, and run evaluation. The only difference is that you use `assert_test()` instead of `evaluate()` to integrate with `pytest`:

```python test_llm_app.py
import pytest
from deepeval.test_case import LLMTestCase
from deepeval.dataset import EvaluationDataset
from deepeval.metrics import AnswerRelevancyMetric
from deepeval import assert_test

dataset = EvaluationDataset()
dataset.pull(alias="YOUR-DATASET-ALIAS")

for golden in dataset.goldens:
    test_case = LLMTestCase(input=golden.input, actual_output=llm_app(golden.input))
    dataset.add_test_case(test_case)

@pytest.mark.parametrize("test_case", dataset.test_cases)
def test_llm_app(test_case: LLMTestCase):
    assert_test(test_case, metrics=[AnswerRelevancyMetric()])
```

Then run with `deepeval test run test_llm_app.py` to execute your tests. Learn more about setting up automated testing in the [Unit-Testing in CI/CD](/llm-evaluation/unit-testing-cicd) section.

## Next Steps

Now that you're familiar with the full dataset lifecycle, time to dive into running evaluations end to end.

<CardGroup cols={2}>
  <Card
    title="Single-Turn Evals"
    icon="arrow-right"
    href="/docs/llm-evaluation/code-driven/single-turn/end-to-end"
  >
    Run end-to-end or component-level evaluations on single-turn interactions.
  </Card>
  <Card
    title="Multi-Turn Evals"
    icon="comments"
    href="/docs/llm-evaluation/code-driven/multi-turn"
  >
    Evaluate conversational AI with multi-turn test cases.
  </Card>
</CardGroup>
