---
subtitle: Learn how to test and use prompts in your LLM app
slug: llm-evaluation/prompt-optimization/using-prompts
---

## Overview

You can pull a prompt version from Confident AI like how you would pull a dataset. It works by:

- Providing Confident AI with the alias and optionally version of the prompt you wish to retrieve
- Confident AI will provide the non-interpolated version of the prompt
- You will then interpolate the variables in code

You should pull prompts once and save it in memory instead of pulling it everytime you need to use it.

## Using Prompt Versions

<Steps>

<Step title="Pull prompt with alias">

Pull your prompt version by providing the `alias` you've defined:

<Tabs>

<Tab title="Python" language="python">

```python
from deepeval.prompt import Prompt

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
prompt.pull()
```

</Tab>

<Tab title="TypeScript" language="typescript">

```typescript
import { Prompt } from "deepeval-ts";

const prompt = new Prompt({ alias: "YOUR-PROMPT-ALIAS" });
await prompt.pull();
```

</Tab>

<Tab title="curL" language="curl">
  <EndpointRequestSnippet endpoint="GET /v1/prompts" />
</Tab>

</Tabs>

<Tip>
  By default, Confident AI will return the latest version of your prompt.
  However, you can also specify the `version` to override this behavior.
</Tip>

</Step>

<Step title="Interpolate variables">

Now that you have your prompt template, interpolate any dynamic variables you may have defined in your prompt version. For example, if this is your prompt version:

<Tabs>

<Tab title="Messages" language="messages">

```json
{
  "role": "system",
  "content": "You are a helpful assistant called {{ name }}. Speak normally like a human."
}
```

And your interpolation type is `{{ variable }}`, interpolating the name (e.g. "Joe") would give you this prompt that is ready for use:

```json
{
  "role": "system",
  "content": "You are a helpful assistant called Joe. Speak normally like a human."
}
```

<Tabs>

<Tab title="Python" language="python">

```python
interpolated_prompt = prompt.interpolate(name="Joe")
```

</Tab>
<Tab title="TypeScript" language="typescript">

```typescript
const interpolatedPrompt = prompt.interpolate({ name: "Joe" });
```

</Tab>

<Tab title="curL" language="curl">

```bash
# Interpolation is done client-side after pulling the prompt
# The API response includes an "interpolationType" field indicating the format:
# - "FSTRING": Use {{ variable }} format (default)
# - "HANDLEBARS": Use {{variable}} format
# Replace variables manually based on the interpolationType in your application code
```

</Tab>

</Tabs>

</Tab>

<Tab title="Text" language="text">

```plaintext
You are a helpful assistant called {{ name }}. Speak normally like a human.
```

And your interpolation type is `{{ variable }}`, interpolating the name (e.g. “Joe”) would give you this prompt that is ready for use:

```plaintext
You are a helpful assistant called Joe. Speak normally like a human.
```

<Tabs>

<Tab title="Python" language="python">

```python
interpolated_prompt = prompt.interpolate(name="Joe")
```

</Tab>
<Tab title="TypeScript" language="typescript">

```typescript
const interpolatedPrompt = prompt.interpolate({ name: "Joe" });
```

</Tab>

<Tab title="curL" language="curl">

```bash
# Interpolation is done client-side after pulling the prompt
# The API response includes an "interpolationType" field indicating the format:
# - "FSTRING": Use {{ variable }} format (default)
# - "HANDLEBARS": Use {{variable}} format
# Replace variables manually based on the interpolationType in your application code
```

</Tab>

</Tabs>

</Tab>
</Tabs>

And if you don't have any variables, you must still use the `interpolate()` method to create a copy of your prompt template to be used in your LLM application.

</Step>

<Step title="Use interpolated prompt">

By now you should have an interpolated prompt version, for example:

<Tabs>

<Tab title="Messages" language="messages">

```json
{
  "role": "system",
  "content": "You are a helpful assistant called Joe. Speak normally like a human."
}
```

Which you can use to generate text from your LLM provider of choice. Here are some examples with OpenAI:

<Tabs>

<Tab title="Python" language="python">

```python main.py {10}
from deepeval.prompt import Prompt
from openai import OpenAI

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
prompt.pull()
interpolated_prompt = prompt.interpolate() # interpolate prompt

response = OpenAI().chat.completions.create(
    model="gpt-4o-mini",
    messages=interpolated_prompt
)

print(response.choices[0].message.content)
```

</Tab>

<Tab title="TypeScript" language="typescript" >

```typescript index.ts {11}
import { Prompt } from "deepeval-ts";
import { OpenAI } from "openai";

const prompt = new Prompt({ alias: "YOUR-PROMPT-ALIAS" });
await prompt.pull();
const interpolatedPrompt = prompt.interpolate(); // interpolate prompt

const openai = new OpenAI();
const response = await openai.chat.completions.create({
  model: "gpt-4o",
  messages: interpolatedPrompt as any[],
});

console.log(response.choices[0].message.content);
```

</Tab>

<Tab title="curL" language="curl">

First, pull the prompt from Confident AI:

<EndpointRequestSnippet endpoint="GET /v1/prompts" />

Then, interpolate the variables and use the interpolated prompt with OpenAI:

```curl
curl -X POST "https://api.openai.com/v1/chat/completions" \
  -H "Authorization: Bearer YOUR_OPENAI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4.1",
    "messages": <YOUR-INTERPOLATED-PROMPT>
  }'
```

</Tab>

</Tabs>

</Tab>

<Tab title="Text" language="text">

```plaintext
You are a helpful assistant called Joe. Speak normally like a human.
```

Which you can use to generate text from your LLM provider of choice. Here are some examples with OpenAI:

<Tabs>

<Tab title="Python" language="python">

```python main.py {10}
from deepeval.prompt import Prompt
from openai import OpenAI

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
prompt.pull()
interpolated_prompt = prompt.interpolate() # interpolate prompt

response = OpenAI().chat.completions.create(
    model="gpt-4o-mini",
    messages={"role": "system", "content": interpolated_prompt}
)

print(response.choices[0].message.content)
```

</Tab>

<Tab title="TypeScript" language="typescript">

```typescript index.ts {11}
import { Prompt } from "deepeval-ts";
import { OpenAI } from "openai";

const prompt = new Prompt({ alias: "YOUR-PROMPT-ALIAS" });
await prompt.pull();
const interpolatedPrompt = prompt.interpolate(); // interpolate prompt

const openai = new OpenAI();
const response = await openai.chat.completions.create({
  model: "gpt-4o-mini",
  messages: [{ role: "system", content: interpolatedPrompt }],
});

console.log(response.choices[0].message.content);
```

</Tab>

<Tab title="curL" language="curl">

First, pull the prompt from Confident AI:

<EndpointRequestSnippet endpoint="GET /v1/prompts" />

Then, interpolate the variables and use the interpolated prompt with OpenAI:

```curl
curl -X POST "https://api.openai.com/v1/chat/completions" \
  -H "Authorization: Bearer YOUR_OPENAI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4.1",
    "messages": <YOUR-INTERPOLATED-PROMPT>
  }'
```

</Tab>

</Tabs>

</Tab>

</Tabs>

</Step>

</Steps>

## Pull Prompts By Label

Previously we saw how we can pull a prompt by supplying the version number. You can also "deploy" a prompt using a **label**, which allows you to select a specific version without defaulting to the latest one:

```python main.py
from deepeval.prompt import Prompt

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
prompt.pull(label="staging")
```

You must manually label each prompt version. [Click here](/docs/llm-evaluation/prompt-optimization/prompt-versioning) to learn how to do so.

## Logging Prompts in Traces

<Steps>

<Step title="Setup tracing">

Attach the `@observe` decorator to functions/methods that make up your agent, and specify type `llm` for your LLM-calling functions.

```python main.py {4}
from deepeval.tracing import observe

@observe(type="llm", model="gpt-4.1")
def your_llm_component():
    ...
```

<Tip>
  Specifying the type is necessary because logging prompts is only available for
  LLM spans.
</Tip>

</Step>

<Step title="Pull and interpolate prompt">

Pull and interpolate the prompt version to use it for LLM generation.

```python main.py {8,9}
from deepeval.tracing import observe
from deepeval.prompt import Prompt
from openai import OpenAI

@observe(type="llm", model="gpt-4.1")
def your_llm_component():
    prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
    prompt.pull()
    interpolated_prompt = prompt.interpolate(name="Joe")
    response = OpenAI().chat.completions.create(model="gpt-4o-mini", messages=interpolated_prompt)
    return response.choices[0].message.content
```

</Step>

<Step title="Execute your function">

Then simply provide the prompt to the `update_llm_span` function.

```python main.py {11}
from deepeval.tracing import observe, update_llm_span
from deepeval.prompt import Prompt
from openai import OpenAI

@observe(type="llm", model="gpt-4.1")
def your_llm_component():
    prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
    prompt.pull()
    interpolated_prompt = prompt.interpolate(name="Joe")
    response = OpenAI().chat.completions.create(model="gpt-4o-mini", messages=interpolated_prompt)
    update_llm_span(prompt=prompt)
    return response.choices[0].message.content
```

<Info>
  Remember to pull the prompt before updating the span, otherwise the prompt
  will not be logged.
</Info>

This will automatically attribute the prompt used to the LLM span.

</Step>

</Steps>

## Prompt caching

Confident AI automatically caches prompts on the client side to **minimize API call latency and ensure prompt availability**, which is especially useful in production environments.

<Tabs>

<Tab title="Cache" language="mermaid">

```mermaid
sequenceDiagram
    participant App as Your Application
    participant DeepEval as DeepEval
    participant Cache as DeepEval Cache
    participant API as Confident AI API

    App->>DeepEval: Pull Prompt
    DeepEval->>Cache: Check cache
    Cache-->>DeepEval: Cached Prompt Data
    DeepEval-->>App: Prompt Data
    Note over Cache,API: Refetching (every 60s)
    Cache->>API: GET /v1/prompts
    API-->>Cache: Update Prompt Cache

```

</Tab>

<Tab title="No Cache" language="mermaid">

```mermaid
sequenceDiagram
    participant App as Your Application
    participant DeepEval as DeepEval
    participant Cache as DeepEval Cache
    participant API as Confident AI API

    Note over App,API: Caching Disabled (refresh=0)
    App->>DeepEval: Pull Prompt
    DeepEval->>Cache: Bypass cache
    Cache->>API: GET /v1/prompts
    API-->>Cache: Prompt Data
    Cache-->>DeepEval: Prompt Data
    DeepEval-->>App: Prompt Data
    Note over Cache,API: Direct API call every time

```

</Tab>

</Tabs>

### Customize refresh rate

By default, the cache is refetched every 60 seconds, where DeepEval will automatically update the cached prompt with the up-to-date version from Confident AI. This can be overridden by setting the `refresh` parameter to a different value. Fetching is done asynchronously, so it will not block your application.

<Tabs>

<Tab title="Python" language="python">

```python main.py {4}
from deepeval.prompt import Prompt

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
prompt.pull(refresh=60)
interpolated_prompt = prompt.interpolate(name="Joe")
```

</Tab>

<Tab title="TypeScript" language="typescript">

```typescript index.ts {4}
import { Prompt } from "deepeval-ts";

const prompt = new Prompt({ alias: "YOUR-PROMPT-ALIAS" });
await prompt.pull({ refresh: 60 });
const interpolatedPrompt = prompt.interpolate({ name: "Joe" });
```

</Tab>

</Tabs>

### Disable Caching

To disable caching, you can set `refresh=0`. This will force an API call every time you pull the prompt, which is particularly useful for development and testing.

<Tabs>

<Tab title="Python" language="python">

```python main.py {4}
from deepeval.prompt import Prompt

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
prompt.pull(refresh=0)
interpolated_prompt = prompt.interpolate(name="Joe")
```

</Tab>

<Tab title="TypeScript" language="typescript">

```typescript index.ts {4}
import { Prompt } from "deepeval-ts";

const prompt = new Prompt({ alias: "YOUR-PROMPT-ALIAS" });
await prompt.pull({ refresh: 0 });
const interpolatedPrompt = prompt.interpolate({ name: "Joe" });
```

</Tab>

</Tabs>
````
