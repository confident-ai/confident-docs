---
subtitle: Create and manage diferent versions of your prompts
slug: llm-evaluation/prompt-optimization/prompt-versioning
---

## Overview

Prompt versioning allows you to optimize and test different versions of your prompts. Managing prompts on Confident AI allows you to:

1. Collaborate and centralize where prompt is stored and edited, even for non-technical team members
2. Pinpoint which version, or even combination of your prompt versions, performed best

There are a million places you can keep your prompts - on GitHub, CSV files, in memory in code, Google Sheets, Notion, or even written in a diary hidden under your table drawer. But only by keeping on prompts on Confident AI can you fully leverage Confident AI's evaluation features.

<Note>
  Prompts are a type of hyperparameter on Confident AI. Other include things
  like models, embedders, top-K, and max tokens. When you run evals against
  prompts that kept on Confident AI, we can tell you which version performs
  best, and later automatically optimize it for you.
</Note>

## Two Types of Prompts

There are two types of prompts you can create:

- **(Single) Text Prompt**: Use this when you need a straightforward, one-off prompt for simple completions.
- **Prompt Message List**: Use this when you need to define multiple messages with specific roles (system, user, assistant) in an OpenAI messages format. This format is ideal for few-shot prompting, where you can start with a system message that sets the context.

<Tip>
  If you ever see a prompt being mentioned without any mention of "message" or
  "list", assume it is a single prompt we're talking about.
</Tip>

## Prompts vs Prompt Versions

In Confident AI, each prompt is identified by a unique `alias`. This `alias` acts as a unique identifier and refers to a single, specific prompt. Different aliases refer to completely separate prompts.

<Info title="Prompt Example">
Suppose you have a prompt with the alias `MyPrompt`. This prompt can have multiple versions, such as `0.0.1`, `0.0.2`, ..., up to `1.0.0`.

```mermaid
flowchart TD
    MyPrompt["Alias: MyPrompt"]
    MyPrompt --> V001["Version 0.0.1"]
    MyPrompt --> V002["Version 0.0.2"]
    MyPrompt --> V003[" ... "]
    MyPrompt --> V100["Version 1.0.0"]
```

</Info>

A **prompt version** represents a different variation of the same prompt. Each `version` is an iteration or improvement of the original prompt, serving the same purpose but with adjustments—similar to how software versions evolve over time.

<Info title="Prompt Version Example">

In addition to the prompt, each prompt version can optionally include the model settings, output type, and output schema.

```mermaid
flowchart LR
    PV["Version 1.0.0"]
    PV --> PT["Prompt"]
    PV --> MS["Model Settings"]
    PV --> OT["Output Type"]
    PV --> OS["Output Schema"]
    PT --> |"Text or Messages"| PT1["You are a helpful assistant..."]
    MS --> |"Temperature, Top-P, etc."| MS1["temperature: 0.7"]
    OT --> |"Response Format"| OT1["JSON, Text, Schema"]
    OS --> |"Structured Output Schema"| OS1["{ name: string }"]
```

</Info>

## Create a Prompt Version

You can create a prompt version in **Project** > **Prompt Studio** through two simple steps:

1. Create a text or messages prompt
2. Edit and commit an initial prompt version in the prompt editor

<Tip>A prompt cannot be both a text and message prompt at the same time.</Tip>

<Tabs>

<Tab title="Messages">

<Frame caption="Create Prompt Messages" background="subtle">

<video
  autoPlay
  loop
  muted
  src="https://confident-docs.s3.us-east-1.amazonaws.com/prompts:create-messages-4k.mp4"
  type="video/mp4"
/>

</Frame>

</Tab>

<Tab title="Text">

<Frame caption="Create Prompt Text" background="subtle">

<video
  autoPlay
  loop
  muted
  src="https://confident-docs.s3.us-east-1.amazonaws.com/prompts:create-text-4k.mp4"
  type="video/mp4"
/>

</Frame>

</Tab>

</Tabs>

Don't forget to **commit** an initial version of your prompt after you're done
editing it.

## Setting Dynamic Variables

You can include variables that can be interpolated dynamically in your LLM application later on. There are four ways currently for you to include variables:

- `{variable}`
- `{{ variable }}`
- `{{variable}}`
- `${variable}`

For example, the `{variable}` interpolation type must follow these rules:

1. No spaces in the variable name
2. Exactly one space between the braces and the variable name

For example:

```python
# ✅ Correct usage:
"Hi, my name is {name}."
"The temperature is {temperature} degrees."
"User input: {user_input}"

# ❌ Incorrect usage:

"Hi, my name is {variable name}." # Spaces in variable name
"Hi, my name is {variable_name }." # Extra spaces around variable name

```

## Using Conditional Logic

Apart from variables, Confident AI also supports [jinja templates](https://realpython.com/primer-on-jinja-templating/), which would allow you to render more complex logic such as conditional if/else blocks:

```txt If/Else
{% if is_admin %}
Welcome back, mighty admin {{ name }}!
{% else %}
Hello {{ name }}, you have regular access.
{% endif %}
```

As well as for loops:

```txt For loop
Shopping List:
{% for item in items %}
- {{ item }}
{% endfor %}
```

<Note>Jinja interpolated prompts are only available for Python users.</Note>

In the next section, we'll show you how to work with prompts you've created on Confident AI.
