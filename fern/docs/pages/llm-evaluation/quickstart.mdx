---
subtitle: 5 min quickstart guide for LLM evaluation
slug: llm-evaluation/quickstart
---

## Overview

Confident AI offers a variety of features for you to test LLM apps in development for a pre-deployment workflow, offering a wide range of features for:

- **End-to-end testing:** Treats your LLM app as a black box. Best for simple architectures such as calling raw model endpoints or lightweight RAG pipelines.
- **Component-level testing:** Built for agentic use cases—debug each agent step and component (planner, tools, memory, retriever, prompts) with granular assertions.
- **Multi-turn evaluation:** Validate full conversations for consistency, state/memory retention, etc.

LLM-as-a-Judge metrics from `deepeval` will be used throughout the platform to auto-score outputs (with or without references) for all of these use cases.

<Success>You don't always need code to run evals.</Success>

You can either run evals locally or remotely on Confident AI, both of which gives you the same functionality:

<CardGroup cols={2}>
  <Card
    title="Local Evals"
    icon="laptop"
    iconType="solid"
  >

    - Run evaluations locally using `deepeval` with full control over metrics
    - Support for custom metrics, DAG, and advanced evaluation algorithms

    **Suitable for:** Python users, development, and pre-deployment workflows

  </Card>
  <Card
    title="Remote Evals"
    icon="cloud"
    iconType="solid"
  >

    - Run evaluations on Confident AI platform with pre-built metrics
    - Integrated with monitoring, datasets, and team collaboration features

    **Suitable for:** Non-python users, online + offline evals for tracing in prod

  </Card>
</CardGroup>

## Key Capabilities

<CardGroup cols={2}>
  <Card
    title="A|B Regression Testing"
    icon="arrows-split-up-and-left"
>

    Catch regressions on different versions of your LLM app with side-by-side test case comparisons

  </Card>
  <Card
    title="Sharable Testing Reports"
    icon="file-text"
  >
    Comprehensive sharable AI testing reports that are sharable through organizations 
  </Card>
  <Card
    title="Prompt and Model Insights"
    icon="gear"
  >
    Find and optimize on the optimal set of prompts, models, and parameters
  </Card>
  <Card
    title="Unit-Testing in CI/CD"
    icon="code"
  >
    Integrate native `pytest` evaluations into your deployment CI/CD pipelines
  </Card>
</CardGroup>

## Run Your First Eval

This examples goes through a **single-turn**, **end-to-end** evaluation example in code.

<Warning>
  You'll need to get your API key as shown in the [setup and
  installation](/docs/setup-and-installation) section before continuing.
</Warning>

<Tabs>

<Tab title="Python" language="python">

<Steps>

  <Step title="Login with API key">

```bash
export CONFIDENT_API_KEY="confident_us..."
```

  </Step>

  <Step title="Create a dataset">

It is mandatory to create a dataset for a proper evaluation workflow.

<Note>
  If a dataset is not possible for your team at this point, setup LLM tracing to
  run ad-hoc evaluations without a dataset instead. Confident AI will generate
  datasets for you automatically this way.
</Note>

<Tabs>

<Tab title="On Platform">
You can create one in the UI under **Project** > **Datasets**, and upload goldens to your dataset via CSV:

<Frame caption="Create Dataset on Confident AI" background="subtle">

<video
  autoPlay
  loop
  muted
  src="https://confident-docs.s3.us-east-1.amazonaws.com/datasets:create-4k.mp4"
  type="video/mp4"
/>

</Frame>

</Tab>

<Tab title="Code">

```python main.py
from deepeval.dataset import EvaluationDataset, Golden
# goldens are what makes up your dataset
goldens = [Golden(input="What's the weather like in SF?")]
# create dataset
dataset = EvaluationDataset(goldens=goldens)
# save to Confident AI
dataset.push(alias="YOUR-DATASET-ALIAS")
```

Done ✅. You should now see your dataset on the platform.

</Tab>

</Tabs>

  </Step>

<Step title="Create a metric">

Create a metric locally in `deepeval`. Here, we're using the `AnswerRelevancyMetric()` for demo purposes.

```python main.py
from deepeval.metrics import AnswerRelevancyMetric

relevancy = AnswerRelevancyMetric() # Using this for the sake of simplicity
```

</Step>
<Step title="Configure evaluation model">

Since all metrics in `deepeval` uses LLM-as-a-Judge, you will also need to configure your LLM judge provider. To use OpenAI for evals:

```bash
export OPENAI_API_KEY="sk-..."
```

<Tip>
  You can also **ANY** model providers since `deepeval` integrates with [all of
  them.](TODO)
</Tip>

</Step>

  <Step title="Create a test run">
  
A test run is a benchmark/snapshot of your LLM app's performance at any point in time. You'll need to:

- Convert all goldens in your dataset into test cases, then
- Use the metric you've created to evaluate each test case

```python main.py
from deepeval.dataset import EvaluationDataset
from deepeval.test_case import LLMTestCase
from deepeval.metrics import AnswerRelevancyMetric
from deepeval import evaluate

# Pull from Confident AI
dataset = EvaluationDataset()
dataset.pull(alias="YOUR-DATASET-ALIAS")

# Create test cases
for golden in dataset.goldens:
    test_case = LLMTestCase(
        input=golden.input,
        actual_output=llm_app(golden.input) # Replace with your LLM app
    )
    dataset.add_test_case(test_case)

# Run an evaluation
evaluate(test_cases=dataset.test_cases, metrics=[AnswerRelevancyMetric()])
```

Lastly, run `main.py` to run your first single-turn, end-to-end evaluation:

```bash
python main.py
```

✅ Done. You just created a first test run with a sharable testing report auto-generated on Confident AI.

      </Step>

  </Steps>

</Tab>

<Tab title="TypeScript" language="typescript">

<Steps>

  <Step title="Login with API key">

```bash
export CONFIDENT_API_KEY="confident_us..."
```

  </Step>

  <Step title="Create a dataset">

It is mandatory to create a dataset for a proper evaluation workflow.

<Note>
  If a dataset is not possible for your team at this point, setup LLM tracing to
  run ad-hoc evaluations without a dataset instead. Confident AI will generate
  datasets for you automatically this way.
</Note>

<Tabs>

<Tab title="On Platform">
You can create one in the UI under **Project** > **Datasets**, and upload goldens to your dataset via CSV:

<Frame caption="Create Dataset on Confident AI" background="subtle">

<video
  autoPlay
  loop
  muted
  src="https://confident-docs.s3.us-east-1.amazonaws.com/datasets:create-4k.mp4"
  type="video/mp4"
/>

</Frame>

</Tab>

<Tab title="Code">

```typescript index.ts
import { EvaluationDataset, Golden } from "deepeval-ts";

async function createDataset() {
  // goldens are what makes up your dataset
  const goldens = [new Golden({ input: "What's the weather like in SF?" })];
  // create dataset
  const dataset = new EvaluationDataset({ goldens });
  // save to Confident AI
  await dataset.push({ alias: "YOUR-DATASET-ALIAS" });
}

createDataset().catch(console.error);
```

Done ✅. You should now see your dataset on the platform.

</Tab>

</Tabs>

  </Step>

<Step title="Create a metric collection">

Create a metric collection under **Project** > **Metrics** > **Collections** with the metrics you wish to use for evals.

<video
  autoPlay
  loop
  muted
  src="https://confident-docs.s3.us-east-1.amazonaws.com/metrics:create-collection-4k.mp4"
  type="video/mp4"
/>
</Step>

  <Step title="Create a test run">
  
A test run is a benchmark/snapshot of your LLM app's performance at any point in time. You'll need to:

- Convert all goldens in your dataset into test cases, then
- Use the metric collection you've created to evaluate each test case

```typescript index.ts maxLines=24
import { EvaluationDataset, LLMTestCase, Golden, evaluate } from "deepeval-ts";

async function runEvaluation() {
  // Pull from Confident AI
  const dataset = new EvaluationDataset();
  await dataset.pull({ alias: "YOUR-DATASET-ALIAS" });

  // Create test cases
  for (const golden of dataset.goldens as Golden[]) {
    const testCase = new LLMTestCase({
      input: golden.input,
      actualOutput: await llmApp(golden.input), // Replace with your LLM app
    });
    dataset.addTestCase(testCase);
  }

  // Run an evaluation
  await evaluate({
    llmTestCases: dataset.testCases,
    metricCollection: "YOUR-METRIC-COLLECTION-NAME",
  });
}

runEvaluation().catch(console.error);
```

Lastly, run `index.ts` to run your first single-turn, end-to-end evaluation:

```bash
tsx index.ts
```

✅ Done. You just created a first test run with a sharable testing report auto-generated on Confident AI.

      </Step>

  </Steps>

</Tab>

</Tabs>

<Frame caption="Testing Report on Confident AI" background="subtle">

<video
  autoPlay
  controls
  muted
  src="https://confident-docs.s3.us-east-1.amazonaws.com/evaluation:single-turn-e2e-report.mp4"
  type="video/mp4"
/>

</Frame>

There are two main pages in a testing report:

- **Overview** - Shows metadata of your test run such as the dataset that was used for testing, average, median, and distribution of each of the metric(s)
- **Test Cases** - Shows all the test cases in your test run, including AI generated summaries of your test bench, and metric data for in-depth debugging and analysis.

When you have two or more test runs, you can also start running A|B regression tests.

## Next Steps

Now that you've run your first **single-turn, end-to-end evaluation**, we'll dive into these core evaluation concepts and techniques:

- **Core Concepts** - Learn what are goldens, test cases and how they form the foundation of end-to-end, component-level, and multi-turn evals.
- **Use Cases** - Understand when to use each evaluation type for agents, AI workflows, RAG, and chatbots.
- **LLM-as-a-Judge Metrics** - Discover 40+ available metrics for different use cases, and how to select them.
