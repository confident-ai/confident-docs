---
subtitle: Build automated prompt management pipelines via the Evals API
slug: llm-evaluation/prompt-management/automate-prompt-management
---

## Overview

Instead of manually creating and updating prompts on the platform, you can automate prompt management via the Evals API. This allows you to:

- Push new prompt commits from your codebase or CI/CD pipeline
- Promote commits to versions programmatically
- Optionally configure model settings, output type, and tools to track alongside prompts
- Integrate prompt management into your development workflow

<Note>
  Most of this page focuses on the core use case: **tracking prompt changes via
  commits**. Model settings, output types, and other configurations are optional
  add-ons for teams who want to manage their entire LLM configuration (prompt +
  model) as a single tracked unit.
</Note>

<Tip>
  If you haven't already, get familiar with [prompt commits and versions on the
  platform](/llm-evaluation/prompt-management/version-prompts) to understand the
  relationship between prompts, commits, versions, and labels.
</Tip>

## Push Prompt Commits

Push a new commit of a prompt to Confident AI. If the prompt alias doesn't exist, it will be created automatically. Every push creates a new commit that tracks your changes.

<Tabs>

<Tab title="Python" language="python">

For **message** prompts:

```python main.py
from deepeval.prompt import Prompt
from deepeval.prompt.api import PromptMessage

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
prompt.push(
    messages=[
        PromptMessage(role="system", content="You are a helpful assistant called {name}."),
    ]
)
```

For **text** prompts:

```python main.py
from deepeval.prompt import Prompt

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
prompt.push(text="You are a helpful assistant called {name}.")
```

You can also specify the interpolation type:

```python main.py
from deepeval.prompt import Prompt
from deepeval.prompt.api import PromptInterpolationType

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
prompt.push(
    text="You are a helpful assistant called {{name}}.",
    interpolation_type=PromptInterpolationType.MUSTACHE
)
```

</Tab>

<Tab title="TypeScript" language="typescript">

For **message** prompts:

```ts index.ts
import { Prompt, PromptMessage } from "deepeval-ts";

const prompt = new Prompt({ alias: "YOUR-PROMPT-ALIAS" });
await prompt.push({
  messages: [
    new PromptMessage({
      role: "system",
      content: "You are a helpful assistant called {name}.",
    }),
  ],
});
```

For **text** prompts:

```ts index.ts
import { Prompt } from "deepeval-ts";

const prompt = new Prompt({ alias: "YOUR-PROMPT-ALIAS" });
await prompt.push({ text: "You are a helpful assistant called {name}." });
```

</Tab>

<Tab title="curL" language="curl">

For **message** prompts:

<EndpointRequestSnippet endpoint="POST /v1/prompts" example="List-Prompt" />

For **text** prompts:

<EndpointRequestSnippet endpoint="POST /v1/prompts" example="Text-Prompt" />

</Tab>

</Tabs>

Each push creates a new commit automatically. When you're ready to mark a commit as a stable release, you can promote it to a version. Version numbers are controlled by Confident AI in the format `00.00.0X`.

## Create a Version

When you're ready to mark a commit as a stable release, you can promote it to a version. Version numbers are automatically assigned by Confident AI in the format `00.00.0X` (e.g., `00.00.01`, `00.00.02`).

<Tabs>

<Tab title="Python" language="python">
```python main.py
from deepeval.prompt import Prompt

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")

# Create a version from the latest commit

prompt.create_version()

# Or create a version from a specific commit

prompt.create_version(hash="COMMIT-HASH")

````

</Tab>

<Tab title="TypeScript" language="typescript">
```ts index.ts
import { Prompt } from "deepeval-ts";

const prompt = new Prompt({ alias: "YOUR-PROMPT-ALIAS" });

// Create a version from the latest commit
await prompt.createVersion();

// Or create a version from a specific commit
await prompt.createVersion({ hash: "COMMIT-HASH" });
````

</Tab>

<Tab title="curL" language="curl">

<EndpointRequestSnippet endpoint="POST /v1/prompts/{alias}/versions" />

</Tab>

</Tabs>

<Warning>
  A new version can only be created for commits made after the most recently
  versioned commit. Commits made before an existing version cannot be promoted
  to a version.
</Warning>

Once a commit is promoted to a version, you can assign labels (like `staging` or `production`) to it. Labels can only exist on versions, not commits.

## Adding Model Configs

<Note>
  Model settings, output type, and tools are **completely optional**. You can
  track and use prompts without any of these — simply pull the prompt and use it
  with whatever model you choose in your code. These options are for teams who
  want to co-locate their model configuration with their prompts.
</Note>

If you want to manage your model configuration alongside your prompts — tracking the prompt + model together in each commit — you can include `model_settings` and `output_type` when pushing.

This is useful when:

- You want to ensure a specific prompt always runs with a specific model and parameters
- You're A/B testing different prompt + model combinations together
- You want to centralize both prompt and model configuration in one place

<Tabs>

<Tab title="Python" language="python">

```python main.py maxLines={0}
from deepeval.prompt import Prompt
from deepeval.prompt.api import (
    PromptMessage,
    ModelSettings,
    ModelProvider,
    OutputType,
    ReasoningEffort,
    Verbosity,
)
from pydantic import BaseModel

class ResponseSchema(BaseModel):
    answer: str
    confidence: float

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")

# Use with push() to create a new commit
prompt.push(
    messages=[
        PromptMessage(role="system", content="You are a helpful assistant."),
    ],
    model_settings=ModelSettings(
        provider=ModelProvider.OPEN_AI,
        name="gpt-4o",
        temperature=0.7,
        max_tokens=1000,
        top_p=0.9,
        frequency_penalty=0.1,
        presence_penalty=0.1,
        stop_sequence=["END"],
        reasoning_effort=ReasoningEffort.MINIMAL,
        verbosity=Verbosity.LOW,
    ),
    output_type=OutputType.SCHEMA,
    output_schema=ResponseSchema,
)
```

</Tab>

<Tab title="TypeScript" language="typescript">

```typescript maxLines={0}
import { Prompt, PromptMessage, OutputType } from "deepeval-ts";

const responseSchema = {
  name: "ResponseSchema",
  fields: {
    answer: "string",
    confidence: "float",
  },
};

const prompt = new Prompt({ alias: "YOUR-PROMPT-ALIAS" });

await prompt.push({
  version: "00.00.01",
  messages: [
    new PromptMessage({
      role: "system",
      content: "You are a helpful assistant.",
    }),
  ],
  modelSettings: {
    provider: "OPEN_AI",
    name: "gpt-4o",
    temperature: 0.7,
    maxTokens: 1000,
    topP: 0.9,
    frequencyPenalty: 0.1,
    presencePenalty: 0.1,
    stopSequence: ["END"],
    reasoningEffort: "MINIMAL",
    verbosity: "LOW",
  },
  outputType: OutputType.SCHEMA,
  outputSchema: responseSchema,
});
```

</Tab>

<Tab title="curL" language="curl">

<EndpointRequestSnippet
  endpoint="PUT /v1/prompts/{alias}/versions/{version}"
  example="Model-Settings"
/>

</Tab>

</Tabs>

## Reference

### Model settings

Model settings include the provider, model name, and model parameters:

| Field    | Type            | Default   | Description                                        |
| -------- | --------------- | --------- | -------------------------------------------------- |
| provider | `ModelProvider` | `OPEN_AI` | The model provider (see supported providers below) |
| name     | `str`           | `None`    | The model name (e.g., "gpt-4o", "claude-3-opus")   |

#### Parameters

Here are all the available parameters you could set:

| Field             | Type              | Default  | Description                                         |
| ----------------- | ----------------- | -------- | --------------------------------------------------- |
| temperature       | `float`           | `0`      | Controls randomness (0-2)                           |
| max_tokens        | `int`             | `None`   | Maximum tokens in the response                      |
| top_p             | `float`           | `1`      | Nucleus sampling parameter                          |
| frequency_penalty | `float`           | `0`      | Penalize repeated tokens (-2 to 2)                  |
| presence_penalty  | `float`           | `0`      | Penalize tokens based on presence (-2 to 2)         |
| stop_sequence     | `List[str]`       | `[]`     | Sequences that stop generation                      |
| reasoning_effort  | `ReasoningEffort` | `MEDIUM` | Reasoning effort level (MINIMAL, LOW, MEDIUM, HIGH) |
| verbosity         | `Verbosity`       | `MEDIUM` | Output verbosity (LOW, MEDIUM, HIGH)                |

<Warning>
  Only include parameters that are valid for your chosen model provider and
  model name. For example, `reasoning_effort` may only apply to certain OpenAI
  models, while other parameters may not be supported by all providers.
  Confident AI does not exhaustively validate which parameter combinations are
  allowed — invalid configurations may result in runtime errors when using the
  prompt in your code.
</Warning>

#### Providers

Here are the list of available model providers:

| Provider      | Description                  |
| ------------- | ---------------------------- |
| `OPEN_AI`     | OpenAI (GPT-4, GPT-4o, etc.) |
| `ANTHROPIC`   | Anthropic (Claude models)    |
| `GEMINI`      | Google Gemini                |
| `VERTEX_AI`   | Google Vertex AI             |
| `BEDROCK`     | Amazon Bedrock               |
| `AZURE`       | Azure OpenAI                 |
| `MISTRAL`     | Mistral AI                   |
| `DEEPSEEK`    | DeepSeek                     |
| `X_AI`        | xAI (Grok)                   |
| `MOONSHOT_AI` | Moonshot AI                  |
| `PERPLEXITY`  | Perplexity                   |
| `PORTKEY`     | Portkey (gateway)            |
| `LITE_LLM`    | LiteLLM (gateway)            |

### Output configurations

You can also configure structured outputs:

| Field         | Type         | Default | Description                                                       |
| ------------- | ------------ | ------- | ----------------------------------------------------------------- |
| output_type   | `OutputType` | `TEXT`  | Output format (TEXT, JSON, or SCHEMA)                             |
| output_schema | `BaseModel`  | `None`  | Pydantic model for structured output (when output_type is SCHEMA) |

**Output Types:**

- `TEXT` - Plain text output
- `JSON` - JSON formatted output
- `SCHEMA` - Structured output validated against a Pydantic schema

### Interpolation types

Specify how variables are interpolated in your prompts:

| Type                  | Syntax           | Example                           |
| --------------------- | ---------------- | --------------------------------- |
| `FSTRING`             | `{variable}`     | `Hello, {name}!`                  |
| `MUSTACHE`            | `{{variable}}`   | `Hello, {{name}}!`                |
| `MUSTACHE_WITH_SPACE` | `{{ variable }}` | `Hello, {{ name }}!`              |
| `DOLLAR_BRACKETS`     | `${variable}`    | `Hello, ${name}!`                 |
| `JINJA`               | `{% ... %}`      | `{% if admin %}Hello!{% endif %}` |

## What about Tools?

You can create and update tools using prompts by pushing prompts with tools. Tools in Confident AI are identified using their names — passing a tool with a new name creates a tool and passing a tool with an existing name updates the tool on the platform. Each push creates a new commit that tracks the tool configuration. Here's how you can create / update tools:

<Tabs>

<Tab title="Python" language="python">

```python
from deepeval.prompt import Prompt, Tool
from deepeval.prompt.api import ToolMode
from pydantic import BaseModel

class ToolInputSchema(BaseModel):
    result: str
    confidence: float

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
tool = Tool(
    name="SearchTool",
    description="Search functionality",
    mode=ToolMode.STRICT,
    structured_schema=ToolInputSchema,
)

# Use with push() to create a new commit with the tool
prompt.push(
    text="This a prompt for a tool using agent",
    tools=[tool]
)

tool_2 = Tool(
    name="SearchTool",
    description="New search functionality",
    mode=ToolMode.STRICT,
    structured_schema=ToolInputSchema,
)

# Create a new commit with the new updated tool using 'push'
prompt.push(
    text="This a prompt for a tool using agent",
    tools=[tool_2]
)
```

</Tab>

<Tab title="TypeScript" language="typescript" >

```typescript
import { Prompt, Tool, ToolMode } from "deepeval-ts";

const responseSchema = {
  name: "ResponseSchema",
  fields: {
    answer: "string",
    confidence: "float",
  },
};

const prompt = new Prompt({ alias: "YOUR-PROMPT-ALIAS" });
const tool = new Tool({
  name = "SearchTool",
  description = "Search functionality",
  mode = ToolMode.STRICT,
  structuredSchema = responseSchema,
});

await prompt.push({
  version: "00.00.01",
  messages: [
    new PromptMessage({
      role: "system",
      content: "You are a helpful assistant.",
    }),
  ],
  tools = [tool],
});

const tool2 = new Tool({
  name = "SearchTool",
  description = "New search functionality",
  mode = ToolMode.STRICT,
  structuredSchema = responseSchema,
});

// Create a new commit with the new updated tool using 'push'
await prompt.push({
  messages: [
    new PromptMessage({
      role: "system",
      content: "You are a helpful assistant.",
    }),
  ],
  tools = [tool2],
});
```

</Tab>

<Tab title="curL" language="curl">

<EndpointRequestSnippet
  endpoint="PUT /v1/prompts/{alias}/versions/{version}"
  example="Tools"
/>

</Tab>

</Tabs>

## Prompts in CI/CD

Automate prompt tracking as part of your CI/CD pipeline. A common pattern is to push prompt commits whenever your prompt files change:

```yaml prompts-ci.yml maxLines={25}
name: Push Prompt Commits

on:
  push:
    paths:
      - "prompts/**"

jobs:
  push-prompts:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install deepeval

      - name: Push prompts
        env:
          CONFIDENT_API_KEY: ${{ secrets.CONFIDENT_API_KEY }}
        run: python scripts/push_prompts.py
```

Your `push_prompts.py` script can read prompt files and push them:

```python scripts/push_prompts.py
from deepeval.prompt import Prompt

# Read your prompt content from file or config
with open("prompts/assistant.txt") as f:
    prompt_text = f.read()

prompt = Prompt(alias="assistant-prompt")
prompt.push(text=prompt_text)

print("Prompt commit pushed successfully!")
```

<Tip>
  Combine automated prompt pushing with [prompt
  labeling](/docs/llm-evaluation/prompt-management/version-prompts#assign-prompt-labels)
  to control which versions are deployed to different environments (e.g.,
  staging, production). Remember that labels can only be assigned to versions,
  so you'll need to promote commits to versions before labeling them.
</Tip>

## Next Steps

Now that you can push prompts programmatically, learn how to pull them into your app for usage.

<Card
  title="Pull Prompts"
  icon="arrow-down-to-bracket"
  href="/docs/llm-evaluation/prompt-management/pull-prompts"
>
  Pull prompt versions into your code for use in your LLM app.
</Card>
