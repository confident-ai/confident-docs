---
subtitle: Build automated prompt versioning pipelines via the Evals API
slug: llm-evaluation/prompt-management/automate-prompt-management
---

## Overview

Instead of manually creating and updating prompts on the platform, you can automate prompt management via the Evals API. This allows you to:

- Push new prompt versions from your codebase or CI/CD pipeline
- Update existing prompt versions programmatically
- Optionally configure model settings, output type, and tools to version prompts + models together
- Integrate prompt versioning into your development workflow

<Note>
  Most of this page focuses on the core use case: **versioning prompts**. Model
  settings, output types, and other configurations are optional add-ons for
  teams who want to manage their entire LLM configuration (prompt + model) as a
  single versioned unit.
</Note>

<Tip>
  If you haven't already, get familiar with [prompt versioning on the
  platform](/llm-evaluation/prompt-management/version-prompts) to understand the
  relationship between prompts, versions, and labels.
</Tip>

## Push Prompt Versions

Push a new version of a prompt to Confident AI. If the prompt alias doesn't exist, it will be created automatically.

<Tabs>

<Tab title="Python" language="python">

For **message** prompts:

```python main.py
from deepeval.prompt import Prompt
from deepeval.prompt.api import PromptMessage

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
prompt.push(
    messages=[
        PromptMessage(role="system", content="You are a helpful assistant called {name}."),
    ]
)
```

For **text** prompts:

```python main.py
from deepeval.prompt import Prompt

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
prompt.push(text="You are a helpful assistant called {name}.")
```

You can also specify the interpolation type:

```python main.py
from deepeval.prompt import Prompt
from deepeval.prompt.api import PromptInterpolationType

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
prompt.push(
    text="You are a helpful assistant called {{name}}.",
    interpolation_type=PromptInterpolationType.MUSTACHE
)
```

</Tab>

<Tab title="TypeScript" language="typescript">

For **message** prompts:

```ts index.ts
import { Prompt, PromptMessage } from "deepeval-ts";

const prompt = new Prompt({ alias: "YOUR-PROMPT-ALIAS" });
await prompt.push({
  messages: [
    new PromptMessage({
      role: "system",
      content: "You are a helpful assistant called {name}.",
    }),
  ],
});
```

For **text** prompts:

```ts index.ts
import { Prompt } from "deepeval-ts";

const prompt = new Prompt({ alias: "YOUR-PROMPT-ALIAS" });
await prompt.push({ text: "You are a helpful assistant called {name}." });
```

</Tab>

<Tab title="curL" language="curl">

For **message** prompts:

<EndpointRequestSnippet endpoint="POST /v1/prompts" example="List-Prompt" />

For **text** prompts:

<EndpointRequestSnippet endpoint="POST /v1/prompts" example="Text-Prompt" />

</Tab>

</Tabs>

Each push creates a new version automatically. Confident AI handles version numbering for you.

## Update Prompt Versions

Update an existing prompt version by specifying the `version` you want to modify. Use `"latest"` to update the most recent version.

<Tabs>

<Tab title="Python" language="python">

For **message** prompts:

```python main.py
from deepeval.prompt import Prompt
from deepeval.prompt.api import PromptMessage

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
prompt.update(
    version="latest",
    messages=[
        PromptMessage(role="system", content="Updated system message."),
    ]
)
```

For **text** prompts:

```python main.py
from deepeval.prompt import Prompt

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")
prompt.update(version="00.00.01", text="Updated prompt text.")
```

</Tab>

<Tab title="TypeScript" language="typescript">

For **message** prompts:

```ts index.ts
import { Prompt, PromptMessage } from "deepeval-ts";

const prompt = new Prompt({ alias: "YOUR-PROMPT-ALIAS" });
await prompt.update({
  version: "latest",
  messages: [
    new PromptMessage({ role: "system", content: "Updated system message." }),
  ],
});
```

For **text** prompts:

```ts index.ts
import { Prompt } from "deepeval-ts";

const prompt = new Prompt({ alias: "YOUR-PROMPT-ALIAS" });
await prompt.update({
  version: "00.00.01",
  text: "Updated prompt text.",
});
```

</Tab>

<Tab title="curL" language="curl">

For **message** prompts:

<EndpointRequestSnippet endpoint="PUT /v1/prompts/{alias}/versions/{version}" example="List-Prompt" />

For **text** prompts:

<EndpointRequestSnippet endpoint="PUT /v1/prompts/{alias}/versions/{version}" example="Text-Prompt" />

</Tab>

</Tabs>

<Warning>
  Updating a prompt version modifies the existing version in place. If you want
  to preserve the original, push a new version instead.
</Warning>

## Adding Model Configs

<Note>
  Model settings, output type, and tools are **completely optional**. You can
  version and use prompts without any of these — simply pull the prompt and use
  it with whatever model you choose in your code. These options are for teams
  who want to co-locate their model configuration with their prompts.
</Note>

If you want to manage your model configuration alongside your prompts — treating the prompt + model as a single versioned unit — you can include `model_settings` and `output_type` when pushing or updating.

This is useful when:

- You want to ensure a specific prompt always runs with a specific model and parameters
- You're A/B testing different prompt + model combinations together
- You want to centralize both prompt and model configuration in one place

<Tabs>

<Tab title="Python" language="python">

```python main.py maxLines={0}
from deepeval.prompt import Prompt
from deepeval.prompt.api import (
    PromptMessage,
    ModelSettings,
    ModelProvider,
    OutputType,
    ReasoningEffort,
    Verbosity,
)
from pydantic import BaseModel

class ResponseSchema(BaseModel):
    answer: str
    confidence: float

prompt = Prompt(alias="YOUR-PROMPT-ALIAS")

# Use with push() to create a new version
prompt.push(
    messages=[
        PromptMessage(role="system", content="You are a helpful assistant."),
    ],
    model_settings=ModelSettings(
        provider=ModelProvider.OPEN_AI,
        name="gpt-4o",
        temperature=0.7,
        max_tokens=1000,
        top_p=0.9,
        frequency_penalty=0.1,
        presence_penalty=0.1,
        stop_sequence=["END"],
        reasoning_effort=ReasoningEffort.MINIMAL,
        verbosity=Verbosity.LOW,
    ),
    output_type=OutputType.SCHEMA,
    output_schema=ResponseSchema,
)

# Or use with update() to modify an existing version
prompt.update(
    version="latest",
    model_settings=ModelSettings(
        provider=ModelProvider.OPEN_AI,
        name="gpt-4o-mini",
        temperature=0.5,
    ),
)
```

</Tab>

<Tab title="TypeScript" language="typescript">

```typescript maxLines={0}
import { Prompt } from "deepeval-ts";

const responseSchema = {
  name: "ResponseSchema",
  fields: {
    answer: "string",
    confidence: "float",
  }
}

const prompt = new Prompt({ alias: "YOUR-PROMPT-ALIAS" });

await prompt.push({
  version: "00.00.01",
  messagesTemplate: [
    new PromptMessage({ role: "system", content: "You are a helpful assistant." }),
  ],
  modelSettings: {
    provider: "OPEN_AI",
    name: "gpt-4o",
    temperature: 0.7,
    maxTokens: 1000,
    topP: 0.9,
    frequencyPenalty: 0.1,
    presencePenalty: 0.1,
    stopSequence: ["END"],
    reasoningEffort: "MINIMAL",
    verbosity: "LOW",
  },
  outputType: OutputType.SCHEMA,
  outputSchema: responseSchema,
});

await prompt.update({
  version: "latest",
  modelSettings: {
    provider: "OPEN_AI",
    name: "gpt-4o",
    temperature: 0.8,
    maxTokens: 2000,
  }
});
```

</Tab>

<Tab title="curL" language="curl">

<EndpointRequestSnippet endpoint="PUT /v1/prompts/{alias}/versions/{version}" example="Model-Settings" />

</Tab>

</Tabs>

## Reference

### Model settings

Model settings include the provider, model name, and model parameters:

| Field    | Type            | Default   | Description                                        |
| -------- | --------------- | --------- | -------------------------------------------------- |
| provider | `ModelProvider` | `OPEN_AI` | The model provider (see supported providers below) |
| name     | `str`           | `None`    | The model name (e.g., "gpt-4o", "claude-3-opus")   |

#### Parameters

Here are all the available parameters you could set:

| Field             | Type              | Default  | Description                                         |
| ----------------- | ----------------- | -------- | --------------------------------------------------- |
| temperature       | `float`           | `0`      | Controls randomness (0-2)                           |
| max_tokens        | `int`             | `None`   | Maximum tokens in the response                      |
| top_p             | `float`           | `1`      | Nucleus sampling parameter                          |
| frequency_penalty | `float`           | `0`      | Penalize repeated tokens (-2 to 2)                  |
| presence_penalty  | `float`           | `0`      | Penalize tokens based on presence (-2 to 2)         |
| stop_sequence     | `List[str]`       | `[]`     | Sequences that stop generation                      |
| reasoning_effort  | `ReasoningEffort` | `MEDIUM` | Reasoning effort level (MINIMAL, LOW, MEDIUM, HIGH) |
| verbosity         | `Verbosity`       | `MEDIUM` | Output verbosity (LOW, MEDIUM, HIGH)                |

<Warning>
  Only include parameters that are valid for your chosen model provider and
  model name. For example, `reasoning_effort` may only apply to certain OpenAI
  models, while other parameters may not be supported by all providers.
  Confident AI does not exhaustively validate which parameter combinations are
  allowed — invalid configurations may result in runtime errors when using the
  prompt in your code.
</Warning>

#### Providers

Here are the list of available model providers:

| Provider      | Description                  |
| ------------- | ---------------------------- |
| `OPEN_AI`     | OpenAI (GPT-4, GPT-4o, etc.) |
| `ANTHROPIC`   | Anthropic (Claude models)    |
| `GEMINI`      | Google Gemini                |
| `VERTEX_AI`   | Google Vertex AI             |
| `BEDROCK`     | Amazon Bedrock               |
| `AZURE`       | Azure OpenAI                 |
| `MISTRAL`     | Mistral AI                   |
| `DEEPSEEK`    | DeepSeek                     |
| `X_AI`        | xAI (Grok)                   |
| `MOONSHOT_AI` | Moonshot AI                  |
| `PERPLEXITY`  | Perplexity                   |
| `PORTKEY`     | Portkey (gateway)            |
| `LITE_LLM`    | LiteLLM (gateway)            |

### Output configurations

You can also configure structured outputs:

| Field         | Type         | Default | Description                                                       |
| ------------- | ------------ | ------- | ----------------------------------------------------------------- |
| output_type   | `OutputType` | `TEXT`  | Output format (TEXT, JSON, or SCHEMA)                             |
| output_schema | `BaseModel`  | `None`  | Pydantic model for structured output (when output_type is SCHEMA) |

**Output Types:**

- `TEXT` - Plain text output
- `JSON` - JSON formatted output
- `SCHEMA` - Structured output validated against a Pydantic schema

### Interpolation types

Specify how variables are interpolated in your prompts:

| Type                  | Syntax           | Example                           |
| --------------------- | ---------------- | --------------------------------- |
| `FSTRING`             | `{variable}`     | `Hello, {name}!`                  |
| `MUSTACHE`            | `{{variable}}`   | `Hello, {{name}}!`                |
| `MUSTACHE_WITH_SPACE` | `{{ variable }}` | `Hello, {{ name }}!`              |
| `DOLLAR_BRACKETS`     | `${variable}`    | `Hello, ${name}!`                 |
| `JINJA`               | `{% ... %}`      | `{% if admin %}Hello!{% endif %}` |

## What about Tools?

Tools can only be defined and configured for prompt versions on the platform UI. After pulling a prompt, you can access its tools programmatically. [Click here](/docs/llm-evaluation/prompt-management/pull-prompts#using-tools) for details on how to use tools in code.

<Note>
  Tool configuration is currently only available through the Confident AI
  platform. Use the Prompt Studio to add, edit, or remove tools from your prompt
  versions.
</Note>

## Prompts in CI/CD

Automate prompt versioning as part of your CI/CD pipeline. A common pattern is to push prompt updates whenever your prompt files change:

```yaml prompts-ci.yml maxLines={25}
name: Push Prompt Versions

on:
  push:
    paths:
      - "prompts/**"

jobs:
  push-prompts:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install deepeval

      - name: Push prompts
        env:
          CONFIDENT_API_KEY: ${{ secrets.CONFIDENT_API_KEY }}
        run: python scripts/push_prompts.py
```

Your `push_prompts.py` script can read prompt files and push them:

```python scripts/push_prompts.py
from deepeval.prompt import Prompt

# Read your prompt content from file or config
with open("prompts/assistant.txt") as f:
    prompt_text = f.read()

prompt = Prompt(alias="assistant-prompt")
prompt.push(text=prompt_text)

print("Prompt pushed successfully!")
```

<Tip>
  Combine automated prompt pushing with [prompt
  labeling](/docs/llm-evaluation/prompt-management/version-prompts#assign-prompt-labels)
  to control which versions are deployed to different environments (e.g.,
  staging, production).
</Tip>

## Next Steps

Now that you can push prompts programmatically, learn how to pull them into your app for usage.

<Card
  title="Pull Prompts"
  icon="arrow-down-to-bracket"
  href="/docs/llm-evaluation/prompt-management/pull-prompts"
>
  Pull prompt versions into your code for use in your LLM app.
</Card>
