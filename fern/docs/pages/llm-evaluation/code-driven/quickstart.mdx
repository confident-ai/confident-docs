---
title: LLM Evaluation Quickstart
subtitle: 5 min quickstart guide for a code-driven LLM evaluation workflow
slug: llm-evaluation/quickstart
---

## Overview

Confident AI offers a variety of features for you to test AI apps using code for a pre-deployment workflow, offering a wide range of features for:

- **Single-turn evaluation:** Input-output as distinct AI interactions.
  - End-to-end: Treats your AI app as a black box.
  - Component-level: Built for agentic use cases—debug each agent step and component (planner, tools, memory, retriever, prompts) with granular assertions.
- **Multi-turn evaluation:** Validate full conversations for consistency, state/memory retention, etc.

You can either run evals via code locally or remotely on Confident AI, both of which gives you the same functionality:

<CardGroup cols={2}>
  <Card
    title="Local Evals"
    icon="laptop"
    iconType="solid"
  >

    - Run evaluations locally using `deepeval` with full control over metrics
    - Support for custom metrics, DAG, and advanced evaluation algorithms

    **Suitable for:** Python users, development, and pre-deployment workflows

  </Card>
  <Card
    title="Remote Evals"
    icon="cloud"
    iconType="solid"
  >

    - Run evaluations on Confident AI platform with pre-built metrics
    - Integrated with monitoring, datasets, and team collaboration features

    **Suitable for:** Non-python users, online + offline evals for tracing in prod

  </Card>
</CardGroup>

## Run Your First Eval

This examples goes through a **single-turn**, **end-to-end** evaluation example in code.

<Warning>
  You'll need to get your API key as shown in the [setup and
  installation](/docs/setup-and-installation) section before continuing.
</Warning>

<Tabs>

<Tab title="Python" language="python">

<Steps>

  <Step title="Login with API key">

```bash
export CONFIDENT_API_KEY="confident_us..."
```

  </Step>

  <Step title="Create a dataset">

It is mandatory to create a dataset for a proper evaluation workflow.

<Note>
  If a dataset is not possible for your team at this point, setup LLM tracing to
  run ad-hoc evaluations without a dataset instead. Confident AI will generate
  datasets for you automatically this way.
</Note>

<Tabs>

<Tab title="Code">

```python main.py
from deepeval.dataset import EvaluationDataset, Golden
# goldens are what makes up your dataset
goldens = [Golden(input="What's the weather like in SF?")]
# create dataset
dataset = EvaluationDataset(goldens=goldens)
# save to Confident AI
dataset.push(alias="YOUR-DATASET-ALIAS")
```

Done ✅. You should now see your dataset on the platform.

</Tab>

<Tab title="On Platform">
You can create one in the UI under **Project** > **Datasets**, and upload goldens to your dataset via CSV:

<Frame caption="Create Dataset on Confident AI" background="subtle">

<video autoPlay loop muted data-video="datasets.create" type="video/mp4" />

</Frame>

</Tab>

</Tabs>

  </Step>

<Step title="Create a metric">

Create a metric locally in `deepeval`. Here, we're using the `AnswerRelevancyMetric()` for demo purposes.

```python main.py
from deepeval.metrics import AnswerRelevancyMetric

relevancy = AnswerRelevancyMetric() # Using this for the sake of simplicity
```

</Step>
<Step title="Configure evaluation model">

Since all metrics in `deepeval` uses LLM-as-a-Judge, you will also need to configure your LLM judge provider. To use OpenAI for evals:

```bash
export OPENAI_API_KEY="sk-..."
```

<Tip>
  You can also use **any** model provider since `deepeval` integrates with [all
  of them.](/settings/organization/model-credentials)
</Tip>

</Step>

  <Step title="Create a test run">
  
A test run is a benchmark/snapshot of your AI app's performance at any point in time. You'll need to:

- Convert all goldens in your dataset into test cases, then
- Use the metric you've created to evaluate each test case

```python main.py
from deepeval.dataset import EvaluationDataset
from deepeval.test_case import LLMTestCase
from deepeval.metrics import AnswerRelevancyMetric
from deepeval import evaluate

# Pull from Confident AI
dataset = EvaluationDataset()
dataset.pull(alias="YOUR-DATASET-ALIAS")

# Create test cases
for golden in dataset.goldens:
    test_case = LLMTestCase(
        input=golden.input,
        actual_output=llm_app(golden.input) # Replace with your AI app
    )
    dataset.add_test_case(test_case)

# Run an evaluation
evaluate(test_cases=dataset.test_cases, metrics=[AnswerRelevancyMetric()])
```

Lastly, run `main.py` to run your first single-turn, end-to-end evaluation:

```bash
python main.py
```

✅ Done. You just created a first test run with a sharable testing report auto-generated on Confident AI.

      </Step>

  </Steps>

</Tab>

<Tab title="TypeScript" language="typescript">

<Steps>

  <Step title="Login with API key">

```bash
export CONFIDENT_API_KEY="confident_us..."
```

  </Step>

  <Step title="Create a dataset">

It is mandatory to create a dataset for a proper evaluation workflow.

<Note>
  If a dataset is not possible for your team at this point, setup LLM tracing to
  run ad-hoc evaluations without a dataset instead. Confident AI will generate
  datasets for you automatically this way.
</Note>

<Tabs>

<Tab title="Code">

```typescript index.ts
import { EvaluationDataset, Golden } from "deepeval-ts";

async function createDataset() {
  // goldens are what makes up your dataset
  const goldens = [new Golden({ input: "What's the weather like in SF?" })];
  // create dataset
  const dataset = new EvaluationDataset({ goldens: goldens });
  // save to Confident AI
  await dataset.push({ alias: "YOUR-DATASET-ALIAS" });
}

createDataset().catch(console.error);
```

Done ✅. You should now see your dataset on the platform.

</Tab>

<Tab title="On Platform">
You can create one in the UI under **Project** > **Datasets**, and upload goldens to your dataset via CSV:

<Frame caption="Create Dataset on Confident AI" background="subtle">

<video autoPlay loop muted data-video="datasets.create" type="video/mp4" />

</Frame>

</Tab>

</Tabs>

  </Step>

<Step title="Create a metric collection">

Create a metric collection under **Project** > **Metrics** > **Collections** with the metrics you wish to use for evals.

<video
  autoPlay
  loop
  muted
  data-video="metrics.createCollection"
  type="video/mp4"
/>
</Step>

  <Step title="Create a test run">
  
A test run is a benchmark/snapshot of your AI app's performance at any point in time. You'll need to:

- Convert all goldens in your dataset into test cases, then
- Use the metric collection you've created to evaluate each test case

```typescript index.ts maxLines=24
import { EvaluationDataset, LLMTestCase, Golden, evaluate } from "deepeval-ts";

async function runEvaluation() {
  // Pull from Confident AI
  const dataset = new EvaluationDataset();
  await dataset.pull({ alias: "YOUR-DATASET-ALIAS" });

  // Create test cases
  for (const golden of dataset.goldens as Golden[]) {
    const testCase = new LLMTestCase({
      input: golden.input,
      actualOutput: await llmApp(golden.input), // Replace with your AI app
    });
    dataset.addTestCase(testCase);
  }

  // Run an evaluation
  await evaluate({
    llmTestCases: dataset.testCases as LLMTestCase[],
    metricCollection: "YOUR-METRIC-COLLECTION-NAME",
  });
}

runEvaluation().catch(console.error);
```

Lastly, run `index.ts` to run your first single-turn, end-to-end evaluation:

```bash
tsx index.ts
```

✅ Done. You just created a first test run with a sharable testing report auto-generated on Confident AI.

      </Step>

  </Steps>

</Tab>

</Tabs>

<Frame caption="Testing Report on Confident AI" background="subtle">

<video
  autoPlay
  controls
  muted
  data-video="evaluation.singleTurnReport"
  type="video/mp4"
/>

</Frame>

There are two main pages in a testing report:

- **Overview** - Shows metadata of your test run such as the dataset that was used for testing, average, median, and distribution of each of the metric(s)
- **Test Cases** - Shows all the test cases in your test run, including AI generated summaries of your test bench, and metric data for in-depth debugging and analysis.

When you have two or more test runs, you can also start running A|B regression tests.

## Next Steps

Now that you've run your first evaluation, dive deeper into single-turn testing:

<CardGroup cols={2}>
  <Card
    title="End-to-End Evals"
    href="/llm-evaluation/single-turn/end-to-end"
    icon="cube"
    iconType="solid"
  >
    Treat your AI app as a black box. Learn how to use LLM tracing for better
    debugging, run remote evals, and log hyperparameters for A|B testing.
  </Card>
  <Card
    title="Component-Level Evals"
    href="/llm-evaluation/single-turn/component-level"
    icon="puzzle-piece"
    iconType="solid"
  >
    Test individual components like retrievers, generators, and tools. Built for
    agentic use cases where you need granular assertions.
  </Card>
</CardGroup>
