---
subtitle: Connect your AI app to run evaluations directly on the platform without code.
slug: settings/project/ai-connections
description: Learn about setting up AI connections
---

AI Connections let you run evaluations directly on the platform by connecting to your AI app via an HTTPS endpoint. Instead of writing code, you can trigger evaluations with a click of a button—Confident AI will call your endpoint with data from your goldens and parse the response.

<Frame caption="Setup AI Connection">
  <img src="https://confident-docs.s3.us-east-1.amazonaws.com/settings:project:ai-connection.png" />
</Frame>

## Setting Up an AI Connection

To create an AI connection:

1. Navigate to **Project Settings** → **AI Connections**
2. Click **New AI Connection**
3. Give it a unique identifying name
4. Click **Save**

<Note>
  Your AI connection won't be usable yet—you still need to configure the
  endpoint, payload, and at minimum the actual output key path.
</Note>

## Configuration Parameters

There are several parameters you'll need to configure in order for your AI connection to work.

### Name

Give your AI connection a unique name to identify it within your project.

### AI App Endpoint

Your AI app **must be accessible via an HTTPS endpoint** that accepts `POST` requests and returns a JSON response containing the actual output of your AI app.

### Payload

Configure the JSON payload that Confident AI sends to your endpoint. You can customize this to match your API's expected structure using values from your goldens.

Available variables:

| Variable                                   | Description                                    | Type       |
| ------------------------------------------ | ---------------------------------------------- | ---------- |
| `golden.input`                             | The input from your golden                     | string     |
| `golden.actual_output`                     | The actual output from your golden             | string     |
| `golden.expected_output`                   | The expected output from your golden           | string     |
| `golden.retrieval_context`                 | The retrieval context from your golden         | string[]   |
| `golden.context`                           | The context from your golden                   | string[]   |
| `golden.expected_tools`                    | The expected tools from your golden            | ToolCall[] |
| `golden.tools_called`                      | The tools called from your golden              | ToolCall[] |
| `golden.additional_metadata`               | Additional metadata from your golden           | object     |
| `conversationalGolden.turns`               | Turn history for multi-turn evals              | Turn[]     |
| `conversationalGolden.context`             | Context for conversational goldens             | string[]   |
| `conversationalGolden.scenario`            | Scenario for conversational goldens            | string     |
| `conversationalGolden.expected_outcome`    | Expected outcome for conversational goldens    | string     |
| `conversationalGolden.user_description`    | User description for conversational goldens    | string     |
| `conversationalGolden.additional_metadata` | Additional metadata for conversational goldens | object     |
| `prompts`                                  | A dictionary of prompts                        | object     |

Example payload:

```json
{
  "input": golden.input,
  "context": golden.context,
  "conversationalContext": conversationalGolden.context,
  "prompts": prompts,
  "turns": conversationalGolden.turns
}
```

<Tip>
  The custom payload feature lets you structure the request to match your
  existing API contract—no need to modify your AI app to accept a specific
  format.
</Tip>

Use `golden.*` variables for single-turn evaluations and `conversationalGolden.*` variables for multi-turn evaluations. See [Prompts](#prompts) for details on how to use the `prompts` dictionary.

### Headers

Add any headers required by your endpoint, such as API keys, authentication tokens, or content type specifications. These headers are sent with every request to your AI app.

### Prompts

Associate prompt versions with your AI connection. When running evaluations, these prompts will be attributed to each test run, letting you trace results back to the prompts used.

The `prompts` variable in your payload is a dictionary where each key maps to an object containing `alias` and `version`:

```json
{
  "system": { "alias": "system-prompt", "version": "1.0.0" },
  "assistant": { "alias": "assistant-prompt", "version": "2.1.0" }
}
```

Here's an example of how your Python endpoint might handle the prompts dictionary:

```python
from deepeval.prompt import Prompt

@app.post("/generate")
def generate(request: dict):
    # Pull different prompt versions using their keys
    system_info = request["prompts"]["system"]
    assistant_info = request["prompts"]["assistant"]

    system_prompt = Prompt(alias=system_info["alias"]).pull(version=system_info["version"])
    assistant_prompt = Prompt(alias=assistant_info["alias"]).pull(version=assistant_info["version"])

    # Use the prompts in your generation
    response = llm.generate(
        system=system_prompt.text,
        assistant=assistant_prompt.text,
        user=request["input"]
    )

    return {"output": response}
```

For more details on working with prompts, see [Prompt Versioning](/docs/llm-evaluation/prompt-optimization/prompt-versioning).

### Actual Output Key Path

A list of strings representing the path to the `actual_output` value in your JSON response. This is required for evaluation to work.

For example, if your endpoint returns:

```json
{
  "response": {
    "output": "Hello, world!"
  }
}
```

Set the key path to `["response", "output"]`.

<Warning>
  You don't need to include `"quotations"` when setting the paths.
</Warning>

### Retrieval Context Key Path

A list of strings representing the path to the `retrieval_context` value in your JSON response. This is optional and only needed if you're using RAG metrics. The value must be a list of strings.

For example, if your endpoint returns:

```json
{
  "response": {
    ...
    "retrieval_context": ["context1", "context2"]
  }
}
```

Set the key path to `["response", "retrieval_context"]`.

### Tool Call Key Path

A list of `ToolCall` representing the path to the `tools_called` value in your JSON response. This is optional and only needed if you're using metrics that require a tool call parameter. The value must be a list of `ToolCall`.

For example, if your endpoint returns:

```json
{
  "response": {
    ...
    "tools_called": [
      {
        "name": "get_weather",
        "description": "Get weather for a location",
        "reasoning": "User asked about the weather in San Francisco",
        "output": "Sunny, 72°F",
        "inputParameters": {"location": "San Francisco"}
      }
    ]
  }
}
```

Set the key path to `["response", "tools_called"]`.

<Tip>
  For more information on the structure of a tool call, refer to the [official
  DeepEval
  documentation](https://deepeval.com/docs/evaluation-test-cases#tools-called).
</Tip>

## Testing Your Connection

After configuring your AI connection, click **Ping Endpoint** to verify everything is set up correctly. You should receive a `200` status response. If not, check the error message and adjust your configuration accordingly.
