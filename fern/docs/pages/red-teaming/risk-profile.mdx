---
subtitle: Risk assessments, top vulnerabilities, incident monitoring, and more.
slug: red-teaming/risk-profile
---

## Overview

The risk profile page on the Confident AI platform is where you can see all your past risk assessments and the insights into your AI application's most critical vulnerabilities, risk issues and assessment pass rates.

## Risk Profile

All individual risk assessments in the risk profile page show you various information about that particular assessment like CVSS score, vulnerability coverage, attack surface, remediation priority, and more. Let's go over each of them in the following sections:

<Frame caption="Risk profile page">
    <img
        data-image="redTeaming.riskAssessment"
        src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"
    />
</Frame>

### CVSS (Common Vulnerability Scoring System)

The **Common Vulnerability Scoring System (CVSS)** is an industry-standard framework for measuring the severity of security vulnerabilities. It provides a numerical score from **0.0 to 10.0**, where higher scores indicate greater severity and potential impact.

Scores are categorized as:

- **0.0** → None
- **0.1 - 3.9** → Low
- **4.0 - 6.9** → Medium
- **7.0 - 8.9** → High
- **9.0 - 10.0** → Critical

The score is calculated based on exploitability and impact to confidentiality, integrity, and availability. Lower scores indicate lower risk while higher scores signal vulnerabilities that should be prioritized.


### Remediation Priority

A classification indicating the urgency of addressing a vulnerability, ranging from **P0** to **P4**. Remediation priorities can be assigned to specific vulnerabilities on the platform itself which allows you prioritize certain security aspects of your AI application.

- **P0** — Critical issues requiring immediate remediation
- **P4** — Low-priority issues suitable for deferred mitigation

### Vulnerability Coverage

Vulnerability coverage represents the breadth of the assessment — specifically, how many distinct vulnerability categories were evaluated. Higher coverage means the system was tested across a wider range of risk domains, resulting in a more comprehensive security evaluation.

Maintaining high vulnerability coverage is recommended, as it ensures your AI application is evaluated across diverse risk categories rather than a limited subset.

### Attack Surface

The attack surface refers to the total set of input vectors, interfaces, and interaction pathways through which a model can be influenced or exploited.

A larger attack surface increases the potential exposure to adversarial behavior if not properly secured. Reducing and tightly controlling the attack surface helps limit opportunities for exploitation and strengthens the overall security posture of the system.

## Test Cases

The Test Cases section displays the complete set of adversarial test cases generated by Confident AI during the risk assessment.

<Frame caption="Risk assessment test cases">
    <img
        data-image="redTeaming.riskAssessmentTestCases"
        src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"
    />
</Frame>

Each test case includes:

- **Input** — The adversarial prompt generated based on a specific vulnerability and attack. This is the input sent to your AI application.
- **Output** — The response produced by your AI application when prompted with the adversarial input.
- **Vulnerability** - The vulnerability that was tested in this particular test cases, this can be any vulnerability like Bias, BFLA, BOLA.
- **Vulnerability Type** - The specific vulnerability types within the vulnerability tested in this test case, for a vulnerability like Bias, a vulnerability type would be gender, race, religion.
- **Attack Method** - This is the adversarial attack method that was used to enhance the base vulnerability attack, some examples for attack methods are Roleplay, Linear Jailbreaking, etc.

Each test cases can have a status of either `passed` or `failed` or `errored`. When a test case's status is shown as `failed`, that implies that your AI application generated an unsafe response to the given adversarial input.

Together, these test cases provide visibility into how your system behaves under targeted adversarial conditions.
