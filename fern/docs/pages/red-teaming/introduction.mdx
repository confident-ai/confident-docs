---
title: Introduction to Red Teaming
subtitle: Proactively identify vulnerabilities and safety issues in your AI applications before they reach production.
slug: red-teaming/introduction
description: Get started with Confident AI's Red Teaming for AI safety and security assessment
---

## Overview

Red Teaming on Confident AI is an **adversarial testing platform** for AI safety and security, and can be done in two ways:

- **No-code** directly in the platform UI, best for security teams, PMs, and compliance officers, or,
- **Code-driven** using the `deepteam` framework, best for engineers and AI red teamers.

The no-code workflow is the more comprehensive option — it leverages the platform's framework builder to produce CVSS scores and full risk profiles. The code-driven workflow uses `deepteam` to orchestrate red teaming runs programmatically.

<Note>

Red Teaming integrates seamlessly with your existing [LLM evaluation](/docs/llm-evaluation/quickstart) and [tracing](/docs/llm-tracing/introduction) workflows on Confident AI.

</Note>

## Key capabilities

Everything you need to operationalize AI red teaming:

<CardGroup cols={2}>
  <Card title="Custom Frameworks" icon="cubes">
    Ships with OWASP Top 10 for LLMs, NIST AI RMF, and more out of the box — or
    build your own with the framework builder.
  </Card>
  <Card title="CVSS Scoring" icon="gauge-high">
    Get standardized vulnerability severity scoring for every risk assessment
    you run.
  </Card>
  <Card title="Compliance Reporting" icon="file-shield">
    Generate reports aligned to regulatory frameworks like EU AI Act, NIST, and
    OWASP Top 10 for LLMs.
  </Card>
  <Card title="CI/CD Integration" icon="code">
    Integrate security testing into your deployment pipelines for continuous
    assessment.
  </Card>
</CardGroup>

## Choose your workflow

Run risk assessments in the UI without any code or use `deepteam` programmatically:

<CardGroup cols={2}>
  <Card
    title="No-Code Assessments"
    icon="browsers"
    iconType="solid"
    href="/docs/red-teaming/no-code-assessments/quickstart"
  >
    <div className='card-link-icon'>
      <Icon icon="arrow-up-right-from-square" />
    </div>

    - Full risk assessments with CVSS scoring
    - Custom framework builder for comprehensive coverage

    **Suitable for:** Security teams, PMs, compliance officers

  </Card>
  <Card
    title="Red Team Using DeepTeam"
    icon="laptop-code"
    iconType="solid"
    href="/docs/red-teaming/code-driven-assessments"
  >
    <div className='card-link-icon'>
      <Icon icon="arrow-up-right-from-square" />
    </div>

    - Programmatic red teaming orchestration via `deepteam`
    - Custom vulnerability and attack development
    - Does not support cloud frameworks or CVSS scoring

    **Suitable for:** Engineers, automated testing

  </Card>
</CardGroup>

<Tip title="Not sure which to pick?">
  Start with **no-code** — it gives you the most comprehensive assessment out of
  the box, including CVSS scores and the framework builder. Use code-driven when
  you need to orchestrate red teaming in CI/CD or develop custom attacks.
  Results from both workflows appear in the same dashboards.
</Tip>

## What you can red team

Test your AI systems across all major vulnerability categories:

<CardGroup cols={3}>
  <Card title="Prompt Injection & Jailbreaks" icon="crosshairs">
    Probe for prompt manipulation, model exploitation, and adversarial bypass
    techniques across 50+ attack patterns.
  </Card>
  <Card title="Bias & Fairness" icon="scale-balanced">
    Assess outputs for bias across protected characteristics and demographic
    groups.
  </Card>
  <Card title="Content Safety" icon="shield-check">
    Evaluate for harmful, toxic, or inappropriate outputs including PII leakage
    and misinformation.
  </Card>
</CardGroup>

## Learn the fundamentals

New to AI red teaming? These concepts will help you get the most out of your setup:

- [What is LLM red teaming?](https://www.confident-ai.com/blog/red-teaming-llms-a-step-by-step-guide) - Step by step guide to red teaming LLMs
- [Risk Profile](/docs/red-teaming/risk-profile) — understand your AI system's attack surface and top vulnerabilities
- [Frameworks & Policies](/docs/red-teaming/framework-policies) — learn how to use AI safety frameworks managed on Confident AI
- [No-Code Quickstart](/docs/red-teaming/no-code-assessments/quickstart) — run your first risk assessment in the platform UI

<AccordionGroup>
  <Accordion title="How is AI Red Teaming different from traditional security testing?">
    AI Red Teaming targets vulnerabilities specific to AI systems — prompt
    injection, model poisoning, adversarial examples — rather than
    infrastructure or application-level security. See our [frameworks
    guide](/docs/red-teaming/framework-policies) for more detail.
  </Accordion>
  <Accordion title="What types of AI systems can be red teamed?">
    All types — conversational AI, RAG systems, multi-agent workflows,
    fine-tuned models, and AI-powered APIs. Each system type has tailored attack
    scenarios and evaluation criteria.
  </Accordion>
  <Accordion title="Do I need security expertise to use Red Teaming?">
    No. The platform provides automated risk assessments, pre-built attack
    scenarios, and step-by-step remediation guidance. Security expertise helps
    with advanced features, but isn't required to get started.
  </Accordion>
</AccordionGroup>
