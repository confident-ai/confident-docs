---
title: Introduction
subtitle: Welcome to Confident AI's Evals API reference.
slug: api-reference/introduction
---

## What is Confident AI's Evals API?

Confident AI enables organizations to offload evaluations, tracing, dataset management, and prompt versioning via a comprehensive RESTFUL **Evals API**.

Whether you're evaluating LLM outputs in production or integrating directly with [DeepEval](https://github.com/confident-ai/deepeval), the platform supports a multi-tenant structure for clean project-level isolation and scalability.

<Tip title="Evaluation metrics via the Evals API are 100% powered by DeepEval">
  DeepEval is one of the most widely adopted LLM evaluation framework in the
  world, with over 10k stars and 20 million daily evaluations.

<Frame background="subtle" caption="â­ DeepEval Star Growth â­">
  [![Star History
  Chart](https://api.star-history.com/svg?repos=confident-ai/deepeval&type=Date)](https://www.star-history.com/#confident-ai/deepeval&Date)
</Frame>

</Tip>

## Quickstart

Here's how you can run your first online evaluation on the Confident AI platform using our [`/v1/evaluate`](/docs/api-reference/evaluation/evaluate-llm) endpoint.

<Steps>
  <Step title="Get Your API Key">
    Get your Project API Key from the Confident AI platform's project settings. Click here to [learn more](/docs/api-reference/authentication).
  </Step>

  <Step title="Create A Metric Collection">
    A metric collection is a list of metrics that are used to evaluate your test cases. It can either consist of single-turn or multi-turn metrics but not both. Learn more about [metric collections here](/docs/metrics/metric-collections).

    <Tabs>
        <Tab title="Create">
            You can create a metric collection using the POST [`/v1/metric-collections`](/docs/api-reference/metric-collections/create-metric-collection) endpoint. Note that all metric collections must have a unique name within your project.

            <EndpointRequestSnippet
                endpoint="POST /v1/metric-collections"
            />

        </Tab>
        <Tab title="List">
            You can see all the metric collections you have in your project using the GET [`/v1/metric-collections`](/docs/api-reference/metric-collections/list-metric-collections) endpoint.

            <EndpointRequestSnippet
                endpoint="GET /v1/metric-collections"
            />

        </Tab>

    </Tabs>

  </Step>
  
  <Step title="Run Your First Eval">
    To run an evaluation, provide the name of the metric collection and the other relevant details based on your use case:

    <Tabs>
        <Tab title="Single Turn">

          Add a list of **llmTestCases** in your request body to run single-turn evaluations. Click here to see the data model of [llmTestCases](/docs/api-reference/evaluation/evaluate-llm#request.body.llmTestCases).
            
          <EndpointRequestSnippet
              endpoint="POST /v1/evaluate"
              example="Single-Turn"
          />

        </Tab>
        <Tab title="Multi Turn">

          Add a list of **conversationalTestCases** in your request body to run multi-turn evaluations. Click here to see the data model of [conversationalTestCases](/docs/api-reference/evaluation/evaluate-llm#request.body.conversationalTestCases).

          <EndpointRequestSnippet
              endpoint="POST /v1/evaluate"
              example="Multi-Turn"
          />

        </Tab>
        <Tab title="Span">
          Get the `uuid` of the span you want to evaluate from the Confident AI platform's Observatory. To learn more about this endpoint [click here](/docs/api-reference/evaluation/evaluate-span).
            
          <EndpointRequestSnippet
              endpoint="POST /v1/evaluate/spans/{uuid}"
              example="Single-Turn"
          />

        </Tab>
        <Tab title="Trace">
          Get the `uuid` of the trace you want to evaluate from the Confident AI platform's Observatory. To learn more about this endpoint [click here](/docs/api-reference/evaluation/evaluate-trace).
            
          <EndpointRequestSnippet
              endpoint="POST /v1/evaluate/traces/{uuid}"
              example="Single-Turn"
          />

        </Tab>
        <Tab title="Thread">
          Get the `id` of the trace you want to evaluate from the Confident AI platform's Observatory. To learn more about this endpoint [click here](/docs/api-reference/evaluation/evaluate-thread).
            
          <EndpointRequestSnippet
              endpoint="POST /v1/evaluate/threads/{id}"
              example="Single-Turn"
          />

        </Tab>
    </Tabs>

  </Step>
</Steps>

The `/v1/evaluate` API endpoint will create a test run on Confident AI and return the following response:

<EndpointResponseSnippet endpoint="POST /v1/evaluate" />

**ðŸŽ‰ Congratulations!** You just ran your first evaluation on Confident AI via the Evals API.

## Test Reports on Confident AI

After running an eval using our Evals API, your test results will be automatically stored on the Confident AI platform in a [comprehensive report format](/docs/llm-evaluation/dashboards/testing-reports). You can also separate the test results using the `TEST-RUN-ID` from the API response.

Here's an example report you'll get on the platform whenever you call the `/v1/evaluate` endpoint:

<Frame caption="Test Reports on Confident AI">
  <video
    src="https://confident-docs.s3.us-east-1.amazonaws.com/evaluation:single-turn-e2e-report.mp4"
    controls
    autoPlay
  />
</Frame>

## FAQs

<AccordionGroup>

    <Accordion title="How the Evals API different from DeepEval?">

    The Evals API provides more low-level control over the DeepEval client and provide benefits that DeepEval alone doesn't offer:

    **Managed Infrastructure**: Serverless evaluations on our managed servers, error handling for metric failures and retries, cost management and billing optimization, automatic scaling based on evaluation volume.

    **Platform Dashboard**: Visual results for each customer dataset, historical tracking and trends, team collaboration features, custom analytics dashboards.

    </Accordion>

    <Accordion title="How is the Evals API different from using the platform?">

    The Evals API and platform serve different use cases in your LLM application development workflow:

    **Platform (Dashboard)**: Use when your engineering teams need to improve an LLM application. It provides visual test case creation, interactive evaluation results, team collaboration features, and built-in dashboards.

    **Evals API**: Use when building an LLM application that needs to automate evaluations for different customers, run evaluations programmatically, build custom dashboards, integrate into existing workflows, or scale across multiple customer environments.

    Both approaches use the same underlying evaluation engine, so you can start with the platform for development and use the API for production automation.

    </Accordion>

    <Accordion title="Who is this for?">

    1. Organizations that need to **scale evaluations across multiple customers or environments** while maintaining visibility into results.
    2. Users that aren't working with Python or Typescript. If users are working with either Python or Typescript, using DeepEval as your client library is highly recommended.

    </Accordion>

</AccordionGroup>
